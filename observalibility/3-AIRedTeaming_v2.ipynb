{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02495886",
   "metadata": {},
   "source": [
    "## üïµüèæ AI Red Teaming Agent for Generative AI Applications üïµüèæ\n",
    "\n",
    "This sample demonstrates how to use Azure AI Evaluation's RedTeam functionality to assess the safety and resilience of AI systems against adversarial prompt attacks.\n",
    "\n",
    "### **Implementation Options**\n",
    "- **SDK Local** - Use the Azure AI Evaluation SDK locally (with PyRIT support)\n",
    "- **Cloud Option** - Use the Azure AI Foundry SDK to execute tests in the cloud\n",
    "\n",
    "Microsoft encourages teams to use the AI Red Teaming Agent to run automated scans throughout the design, development, and pre-deployment stage:\n",
    "\n",
    "- Design: Picking out the safest foundational model on your use case.\n",
    "- Development: Upgrading models within your application or creating fine-tuned models for your specific application.\n",
    "- Pre-deployment: Before deploying GenAI applications to productions.\n",
    "\n",
    "### **Evaluations**\n",
    "AI Red Teaming Agent leverages [Risk and Safety Evaluations](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/evaluation-metrics-built-in?tabs=warning#risk-and-safety-evaluators) to help identify potential safety issues across different risk categories (violence, hate/unfairness, sexual content, self-harm) combined with attack strategies of varying complexity levels from [PyRIT](https://github.com/Azure/PyRIT), Microsoft AI Red Teaming team's open framework for automated AI red teaming.\n",
    "- **Automated scans** for content risks: first, you can automatically scan your model and application endpoints for safety risks by simulating adversarial probing.\n",
    "- **Evaluate probing success:** next, you can evaluate and score each attack-response pair to generate insightful metrics such as Attack Success Rate (ASR).\n",
    "- **Reporting and logging:** finally, you can generate a score card of the attack probing techniques and risk categories to help you decide if the system is ready for deployment. Findings can be logged, monitored, and tracked over time directly in Azure AI Foundry, ensuring compliance and continuous risk mitigation.\n",
    "\n",
    "üìö **For complete details on AI Red Teaming Agent, visit:**  \n",
    "**[Azure AI Foundry red teaming agent overview](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/ai-red-teaming-agent)**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0c0ebe",
   "metadata": {},
   "source": [
    "## üïµüèæ Red Teaming Agent - Notebook Overview üïµüèæ\n",
    "\n",
    "This notebook walks through how to use Azure AI Evaluation's AI Red Teaming Agent functionality to assess the safety and resilience of AI systems against adversarial prompt attacks.\n",
    "\n",
    "## What This Notebook Does:\n",
    "1. **Setup & Environment** - Configure setup and environment file\n",
    "2. **Local Evaluation** - Runs red teaming scans locally (Azure AI Evaluation SDK)\n",
    "3. **Cloud Evaluation** - Runs red teaming scans in the cloud (Azure AI Foundry SDK)\n",
    "4. (T) **Copilot-studio created agents option** - walks through how to use Azure AI Evaluation's AI Red Teaming Agent functionality to assess the safety and resilience of **Copilot Studio-created agents* against adversarial prompt attacks.\n",
    "\n",
    "## Key Features:\n",
    "‚úÖ **Local Evaluations** - Run scan and specify risk categories \n",
    "\n",
    "‚úÖ **Cloud Integration** - Create red teaming run and view results\n",
    "\n",
    "‚úÖ **Strategy and Planning** - potential strategies for planning how to set up and manage red teaming for responsible AI (RAI) risks throughout the large language model (LLM) product life cycle.\n",
    "\n",
    "‚úÖ **Error Handling** - Robust fallbacks and clear status reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11975d08",
   "metadata": {},
   "source": [
    "## 1. Initialization and Setup \n",
    "**Prerequisites**\n",
    "- An Azure AI Foundry project or hubs based project. To learn more, see [Create a project](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/create-projects).\n",
    "- Create and connect your storage account to your Azure AI Foundry project at the resource level. There are two ways you can do this. You can use a Bicep template, which provisions and connects a storage account to your Foundry project with key authentication. You can also manually create and provision access to your storage account in the Azure portal.\n",
    "- Make sure the connected storage account has access to all projects.\n",
    "- If you connected your storage account with Microsoft Entra ID, make sure to give managed identity Storage Blob Data Owner permissions to both your account and the Foundry project resource in the Azure portal.\n",
    "\n",
    "- A `.env` file containing `PROJECT_CONNECTION_STRING` (and optionally `MODEL_DEPLOYMENT_NAME`).\n",
    "- A local environment with `redteam`, `azure-ai-identity`, `azure-ai-evaluation` packages installed.\n",
    "\n",
    "**Note:**\n",
    "Currently, AI Red Teaming Agent is only available in a few regions. Ensure your Azure AI Project is located in the following supported regions:\n",
    "\n",
    "- East US2\n",
    "- Sweden Central\n",
    "- France Central\n",
    "- Switzerland West\n",
    "\n",
    "**What we do**:\n",
    "- Load environment variables.\n",
    "- Initialize `RedTeam` agent (currently in preview, not recommend for production workloads).\n",
    "- Run scan on simple callback target.\n",
    "- Specify risk categories to cover in scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2300d587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# May need to install the Azure AI Evaluation SDK red teaming package \n",
    "## If you haven't installed the Azure AI Evaluation SDK with red teaming support, run the following command\n",
    "#!pip install azure-ai-evaluation[redteam]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b889daf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "    ## Import libraries and check connection and tenant ID\n",
    "# Import necessary libraries\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "# Azure imports\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.evaluation.red_team import RedTeam, RiskCategory\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Load environment variables\n",
    "notebook_path = Path().absolute()\n",
    "env_path = notebook_path.parent.parent / '.env'  # Adjust path as needed\n",
    "load_dotenv(env_path)\n",
    "# Get connection string and tenant ID from environment variables\n",
    "connection_string = os.environ.get(\"PROJECT_CONNECTION_STRING\")\n",
    "tenant_id = os.environ.get(\"TENANT_ID\")\n",
    "if not connection_string:\n",
    "    raise ValueError(\"üö® PROJECT_CONNECTION_STRING not set in .env.\")\n",
    "# Confirm tenant ID is set\n",
    "print(f\"üîë Using Tenant ID: {tenant_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb63e83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate your AI Red Teaming Agent\n",
    "red_team_agent = RedTeam(\n",
    "    azure_ai_project=connection_string, # required\n",
    "    credential=DefaultAzureCredential() # required\n",
    ")\n",
    "\n",
    "# A simple example application callback function that always returns a fixed response\n",
    "def simple_callback(query: str) -> str:\n",
    "    return \"I'm an AI assistant that follows ethical guidelines. I cannot provide harmful content.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2d5598",
   "metadata": {
    "id": "3-Local-Evaluation"
   },
   "source": [
    "## 2. Run Scan Locally\n",
    "**üîç Local Evaluation Simple Callback**\n",
    "\n",
    "This example generates a default set of 10 attack prompts for each of the default set of four risk categories (violence, sexual, hate and unfairness, and self-harm) to result in a total of 40 rows of attack prompts to be generated and sent to your target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eebbae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'results' folder if it doesn't exist\n",
    "results_folder = Path(\"./results\")\n",
    "results_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define the path for the red team results file\n",
    "red_team_results_path = results_folder / \"red_team_results1.jsonl\"\n",
    "\n",
    "# Runs a red teaming scan on the simple callback target\n",
    "red_team_result = await red_team_agent.scan(\n",
    "    target=simple_callback,\n",
    "    output_path=red_team_results_path)\n",
    "\n",
    "# Note: the default iterations uses 10 objectives, so this may take 2-5 min.\n",
    "# The scan results are also viewable in the Azure AI Foundry project UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac493b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick view of results, also saved in filepath specified above\n",
    "# Define the path to the results file\n",
    "results_path = Path(\"./results/red_team_results1.jsonl\")\n",
    "\n",
    "# Read and display each line (each line is a JSON object)\n",
    "with results_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        result = json.loads(line)\n",
    "        print(json.dumps(result, indent=2))  # Pretty print each result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0b9229",
   "metadata": {},
   "source": [
    "**üîç Specify Risk Categories**\n",
    "\n",
    "Optionally, you can specify which risk categories of content risks you want to cover with risk_categories parameter and define the number of prompts covering each risk category with num_objectives parameter.\n",
    "\n",
    "*AI Red Teaming Agent only supports single-turn interactions in text-only scenarios.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f594c353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying risk categories and number of attack objectives per risk categories you want the AI Red Teaming Agent to cover\n",
    "red_team_agent = RedTeam(\n",
    "    azure_ai_project=connection_string, # required\n",
    "    credential=DefaultAzureCredential(), # required\n",
    "    risk_categories=[ # optional, defaults to all four risk categories\n",
    "        RiskCategory.Violence #,\n",
    "        #RiskCategory.HateUnfairness,\n",
    "        #RiskCategory.Sexual,\n",
    "        #RiskCategory.SelfHarm\n",
    "    ], \n",
    "    num_objectives=1, # optional, defaults to 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37577feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the red team scan called \"Basic-Callback-Scan\" with limited scope for this basic example\n",
    "# This will test 5 objective prompt for specified Violence category with the Flip strategy\n",
    "# Import the necessary attack strategy\n",
    "from azure.ai.evaluation.red_team import AttackStrategy\n",
    "\n",
    "# Define the path for the red team results file\n",
    "red_team_results_path = results_folder / \"red_team_results2.jsonl\"\n",
    "# Runs a red teaming scan on the simple callback target with specified attack strategies\n",
    "result = await red_team_agent.scan(\n",
    "    target=simple_callback,\n",
    "    scan_name=\"Basic-Flip-Scan\",\n",
    "    attack_strategies=[AttackStrategy.Flip],\n",
    "    output_path=red_team_results_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e198092",
   "metadata": {},
   "source": [
    "**Complex callback**: A more complex callback that is aligned to the OpenAI Chat Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e104d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more complex callback function that handles conversation state\n",
    "async def advanced_callback(messages, stream=False, session_state=None, context=None):\n",
    "    # Extract the latest message from the conversation history\n",
    "    messages_list = [{\"role\": message.role, \"content\": message.content} \n",
    "                    for message in messages]\n",
    "    latest_message = messages_list[-1][\"content\"]\n",
    "    \n",
    "    # In a real application, you might process the entire conversation history\n",
    "    # Here, we're just simulating a response\n",
    "    response = \"I'm an AI assistant that follows safety guidelines. I cannot provide harmful content.\"\n",
    "    \n",
    "    # Format the response to follow the expected chat protocol format\n",
    "    formatted_response = {\n",
    "        \"content\": response,\n",
    "        \"role\": \"assistant\"\n",
    "    }\n",
    "    \n",
    "    return {\"messages\": [formatted_response]}\n",
    "\n",
    "red_team_result = await red_team_agent.scan(target=advanced_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ae3439",
   "metadata": {},
   "source": [
    "üìà **Results from automated scans**\n",
    "The key metric for assessing your results is the Attack Success Rate (ASR), which measures the percentage of attacks that successfully elicit undesirable responses from your AI system. This can ve viewed in Evaluation section in AI Foundry or as the output path defined in the scan.\n",
    "\n",
    "The output_path that captures a JSON file that represents a scorecard of your results for using in your own reporting tool or compliance platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0710ae56",
   "metadata": {},
   "source": [
    "## 3. Run Scan in Cloud\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d525400",
   "metadata": {},
   "source": [
    "**‚òÅÔ∏è Cloud Evaluation**\n",
    "\n",
    "Running scans in the cloud allows for pre-deployment AI red teaming runs on larger combinations of attack strategies and risk categories for a fuller analysis. View results in Azure AI Foundry project for tracking and collaboration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965f796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud Evaluation - Fixed using Official Microsoft Documentation\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "print(\"‚òÅÔ∏è Setting up Cloud Evaluation following official documentation...\")\n",
    "\n",
    "# Step 1: Install and import required packages\n",
    "try:\n",
    "    from azure.ai.projects import AIProjectClient\n",
    "    from azure.ai.projects.models import (\n",
    "        RedTeam,\n",
    "        AzureOpenAIModelConfiguration,\n",
    "        AttackStrategy,\n",
    "        RiskCategory,\n",
    "    )\n",
    "    print(\"‚úÖ Azure AI Projects SDK found\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Installing azure-ai-projects...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"azure-ai-projects>=1.0.0b4\"])\n",
    "    \n",
    "    from azure.ai.projects import AIProjectClient\n",
    "    from azure.ai.projects.models import (\n",
    "        RedTeam,\n",
    "        AzureOpenAIModelConfiguration,\n",
    "        AttackStrategy,\n",
    "        RiskCategory\n",
    "    )\n",
    "    print(\"‚úÖ Packages installed successfully\")\n",
    "\n",
    "# Step 2: Configuration using official environment variable names\n",
    "PROJECT_ENDPOINT = os.environ.get(\"PROJECT_CONNECTION_STRING\")\n",
    "MODEL_ENDPOINT = os.environ.get(\"AZURE_OPENAI_ENDPOINT\") \n",
    "MODEL_API_KEY = os.environ.get(\"AZURE_OPENAI_API_KEY\")\n",
    "MODEL_DEPLOYMENT_NAME = os.environ.get(\"MODEL_DEPLOYMENT_NAME\")\n",
    "\n",
    "\n",
    "print(f\"üè¢ Project Endpoint: {PROJECT_ENDPOINT}\")\n",
    "print(f\"ü§ñ Model Deployment: {MODEL_DEPLOYMENT_NAME}\")\n",
    "print(f\"üîó Model Endpoint: {MODEL_ENDPOINT}\")\n",
    "\n",
    "if not PROJECT_ENDPOINT:\n",
    "    print(\"‚ö†Ô∏è Missing AZURE_AI_PROJECT_ENDPOINT in .env file\")\n",
    "    cloud_result = None\n",
    "else:\n",
    "    try:\n",
    "        # Step 3: Authentication using DefaultAzureCredential\n",
    "        print(\"üîê Setting up authentication...\")\n",
    "        credential = DefaultAzureCredential()\n",
    "        \n",
    "        # Step 4: Create AI Project Client\n",
    "        with AIProjectClient(endpoint=PROJECT_ENDPOINT, credential=credential) as project_client:\n",
    "            print(\"üåê AI Project Client created successfully\")\n",
    "           \n",
    "            # Step 5: Create Red Teaming Agent\n",
    "            print(\"ü§ñ Creating Red Teaming Agent...\")\n",
    "            # Define target configuration for Azure OpenAI model\n",
    "            target_config = AzureOpenAIModelConfiguration(model_deployment_name=MODEL_DEPLOYMENT_NAME)\n",
    "\n",
    "            # Step 6: Create Red Teaming Agent\n",
    "            red_team_agent = RedTeam(\n",
    "                attack_strategies=[AttackStrategy.BASE64],\n",
    "                risk_categories=[RiskCategory.VIOLENCE],\n",
    "                display_name=\"red-team-cloud-run\",\n",
    "                target=target_config,\n",
    "            )\n",
    "\n",
    "            # Step 7: Set headers for model configuration\n",
    "            headers = {\n",
    "                \"model-endpoint\": MODEL_ENDPOINT,\n",
    "                \"api-key\": MODEL_API_KEY,\n",
    "            }\n",
    "\n",
    "            print(\"üöÄ Submitting Red Teaming scan...\")\n",
    "            red_team_response = project_client.red_teams.create(red_team=red_team_agent, headers=headers)\n",
    "\n",
    "            print(\"‚è≥ Checking scan status...\")\n",
    "            time.sleep(2)\n",
    "            get_red_team_response = project_client.red_teams.get(name=red_team_response.name)\n",
    "            print(f\"üìä Red Team scan status: {get_red_team_response.status}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Red Teaming setup failed: {e}\")\n",
    "        print(f\"üìã Error type: {type(e).__name__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7325aef2",
   "metadata": {},
   "source": [
    "Once your AI red teaming run is finished running, you can view your results in your Azure AI Foundry project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
