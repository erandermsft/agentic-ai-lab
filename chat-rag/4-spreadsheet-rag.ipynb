{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a06b2c6a",
   "metadata": {},
   "source": [
    "# ðŸ“Š Spreadsheet RAG with SpreadsheetChunker\n",
    "\n",
    "This notebook demonstrates how to use the **SpreadsheetChunker** to process Excel files and create chunks suitable for RAG (Retrieval-Augmented Generation).\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Load and chunk local Excel files\n",
    "- Two chunking strategies:\n",
    "  - **Row-by-row chunking**: Creates individual chunks for each data row\n",
    "  - **Sheet-level chunking**: Creates one chunk per worksheet with AI-generated summaries\n",
    "- Prepare spreadsheet data for embedding and vector search\n",
    "\n",
    "> **Note**: This builds on the concepts from [3-basic-rag.ipynb](3-basic-rag.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3af8d61",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's install the required packages for working with Excel files and token estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67ea6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openpyxl tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8266310c",
   "metadata": {},
   "source": [
    "## 2. Row-by-Row Chunking ðŸ“\n",
    "\n",
    "Let's demonstrate how to use the `SpreadsheetChunker` to process a local Excel file and create chunks suitable for RAG.\n",
    "\n",
    "**What this does:**\n",
    "- Loads a local Excel file from your filesystem\n",
    "- Chunks the data by individual rows\n",
    "- Optionally includes headers in each chunk\n",
    "- Prepares data for embedding and indexing\n",
    "\n",
    "**Note:** Make sure you have a sample Excel file ready. You can create one with sample data or use any existing `.xlsx` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26d7ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple demo: Load a local Excel file and chunk it\n",
    "from chunkers import SpreadsheetChunker\n",
    "from pathlib import Path\n",
    "\n",
    "# Replace with your actual Excel file path\n",
    "excel_file_path = \"docs/Weekly time record small business.xlsx\"  # Change this to your file\n",
    "\n",
    "# Helper function to create data dict for local files\n",
    "def create_local_file_data(file_path: str):\n",
    "    \"\"\"Create data dictionary for SpreadsheetChunker from local file path.\"\"\"\n",
    "    file_path = Path(file_path).resolve()  # Convert to absolute path\n",
    "    \n",
    "    # Read file bytes\n",
    "    with open(file_path, 'rb') as f:\n",
    "        file_bytes = f.read()\n",
    "    \n",
    "    return {\n",
    "        'documentUrl': file_path.as_uri(),  # file:// URL (needs absolute path)\n",
    "        'documentSasToken': '',  # No SAS token for local files\n",
    "        'fileName': file_path.name,\n",
    "        'documentBytes': file_bytes,  # Actual file content\n",
    "        'documentContent': ''  # Empty for Excel files\n",
    "    }\n",
    "\n",
    "# Create the data dictionary\n",
    "data = create_local_file_data(excel_file_path)\n",
    "\n",
    "# Create chunker with local file data\n",
    "chunker = SpreadsheetChunker(\n",
    "    data=data,\n",
    "    max_chunk_size=1536,\n",
    "    chunking_by_row=True,  # or False for sheet-based chunking\n",
    "    include_header_in_chunks=True\n",
    ")\n",
    "\n",
    "# Get chunks\n",
    "chunks = chunker.get_chunks()\n",
    "\n",
    "# Display results\n",
    "print(f\"âœ… Created {len(chunks)} chunks from {excel_file_path}\")\n",
    "for chunk in chunks[:10]:  # Show first 10 chunks\n",
    "    if \"summary\" in chunk:\n",
    "        print(chunk[\"summary\"])\n",
    "    print(f\"\\nðŸ“„ Chunk {chunk['chunk_id']}: {chunk['title']}\")\n",
    "    print(chunk['content'][:300] + \"...\" if len(chunk['content']) > 300 else chunk['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355f66c6",
   "metadata": {},
   "source": [
    "## 3. Sheet-Level Chunking with AI Summaries ðŸ¤–\n",
    "\n",
    "Instead of creating individual chunks for each row, let's try chunking **by sheet**. This approach:\n",
    "\n",
    "- Creates one chunk per worksheet\n",
    "- Uses AI to generate a summary of the entire sheet\n",
    "- Better for high-level understanding of spreadsheet data\n",
    "- Reduces total number of chunks for large spreadsheets\n",
    "\n",
    "Set `chunking_by_row=False` to enable this mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275224d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chunker with sheet-level chunking (with AI summaries)\n",
    "chunker_by_sheet = SpreadsheetChunker(\n",
    "    data=data,\n",
    "    max_chunk_size=1536,\n",
    "    chunking_by_row=False,  # Chunk by sheet instead of by row\n",
    "    include_header_in_chunks=False  # Not applicable for sheet-level chunking\n",
    ")\n",
    "\n",
    "# Get chunks\n",
    "sheet_chunks = chunker_by_sheet.get_chunks()\n",
    "\n",
    "# Display results\n",
    "print(f\"âœ… Created {len(sheet_chunks)} chunks (one per sheet) from {excel_file_path}\")\n",
    "for chunk in sheet_chunks:\n",
    "    print(f\"\\nðŸ“„ Chunk {chunk['chunk_id']}: {chunk['title']}\")\n",
    "    print(f\"\\nðŸ¤– AI Summary:\\n{chunk.get('summary', 'No summary available')}\")\n",
    "    print(f\"\\nðŸ“Š Table Data (first 500 chars):\\n{chunk['content'][:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9a45aa",
   "metadata": {},
   "source": [
    "## 4. Comparing Chunking Strategies\n",
    "\n",
    "Let's compare the two approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a03357",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“Š Chunking Strategy Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Row-by-row chunking: {len(chunks)} chunks\")\n",
    "print(f\"Sheet-level chunking: {len(sheet_chunks)} chunks\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nðŸ’¡ Use Cases:\")\n",
    "print(\"\\nRow-by-row:\")\n",
    "print(\"  âœ… Granular search across individual records\")\n",
    "print(\"  âœ… When each row represents a distinct entity\")\n",
    "print(\"  âœ… Better for precise retrieval of specific data points\")\n",
    "print(\"\\nSheet-level:\")\n",
    "print(\"  âœ… High-level understanding of data structure\")\n",
    "print(\"  âœ… When you need summaries of entire datasets\")\n",
    "print(\"  âœ… Reduces number of chunks for large spreadsheets\")\n",
    "print(\"  âœ… Better for understanding overall trends and patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c7277e",
   "metadata": {},
   "source": [
    "## 5. Next Steps: Integrating with RAG\n",
    "\n",
    "Now that you have chunks, you can:\n",
    "\n",
    "1. **Generate embeddings** for each chunk using the embedding model\n",
    "2. **Upload to Azure AI Search** with vector search capability\n",
    "3. **Query the index** to retrieve relevant spreadsheet data\n",
    "4. **Use with chat model** to answer questions about your spreadsheet data\n",
    "\n",
    "See [3-basic-rag.ipynb](3-basic-rag.ipynb) for the complete RAG pipeline implementation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd063230",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "You've learned how to:\n",
    "\n",
    "âœ… Load and process local Excel files  \n",
    "âœ… Use row-by-row chunking for granular data access  \n",
    "âœ… Use sheet-level chunking with AI summaries  \n",
    "âœ… Prepare spreadsheet data for RAG pipelines  \n",
    "\n",
    "Choose the chunking strategy that best fits your use case! ðŸŽ¯"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
