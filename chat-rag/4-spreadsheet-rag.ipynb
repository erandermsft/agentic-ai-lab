{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a06b2c6a",
   "metadata": {},
   "source": [
    "# 📊 Spreadsheet RAG with SpreadsheetChunker\n",
    "\n",
    "This notebook demonstrates how to use the **SpreadsheetChunker** to process Excel files and create chunks suitable for RAG (Retrieval-Augmented Generation).\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Load and chunk local Excel files\n",
    "- Two chunking strategies:\n",
    "  - **Row-by-row chunking**: Creates individual chunks for each data row\n",
    "  - **Sheet-level chunking**: Creates one chunk per worksheet with AI-generated summaries\n",
    "- Prepare spreadsheet data for embedding and vector search\n",
    "\n",
    "> **Note**: This builds on the concepts from [3-basic-rag.ipynb](3-basic-rag.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3af8d61",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's install the required packages for working with Excel files and token estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67ea6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openpyxl tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8266310c",
   "metadata": {},
   "source": [
    "## 2. Row-by-Row Chunking 📝\n",
    "\n",
    "Let's demonstrate how to use the `SpreadsheetChunker` to process a local Excel file and create chunks suitable for RAG.\n",
    "\n",
    "**What this does:**\n",
    "- Loads a local Excel file from your filesystem\n",
    "- Chunks the data by individual rows\n",
    "- Optionally includes headers in each chunk\n",
    "- Prepares data for embedding and indexing\n",
    "\n",
    "**Note:** Make sure you have a sample Excel file ready. You can create one with sample data or use any existing `.xlsx` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26d7ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple demo: Load a local Excel file and chunk it\n",
    "from chunkers import SpreadsheetChunker\n",
    "from pathlib import Path\n",
    "\n",
    "# Replace with your actual Excel file path\n",
    "excel_file_path = \"docs/Weekly time record small business.xlsx\"  # Change this to your file\n",
    "\n",
    "# Helper function to create data dict for local files\n",
    "def create_local_file_data(file_path: str):\n",
    "    \"\"\"Create data dictionary for SpreadsheetChunker from local file path.\"\"\"\n",
    "    file_path = Path(file_path).resolve()  # Convert to absolute path\n",
    "    \n",
    "    # Read file bytes\n",
    "    with open(file_path, 'rb') as f:\n",
    "        file_bytes = f.read()\n",
    "    \n",
    "    return {\n",
    "        'documentUrl': file_path.as_uri(),  # file:// URL (needs absolute path)\n",
    "        'documentSasToken': '',  # No SAS token for local files\n",
    "        'fileName': file_path.name,\n",
    "        'documentBytes': file_bytes,  # Actual file content\n",
    "        'documentContent': ''  # Empty for Excel files\n",
    "    }\n",
    "\n",
    "# Create the data dictionary\n",
    "data = create_local_file_data(excel_file_path)\n",
    "\n",
    "# Create chunker with local file data\n",
    "chunker = SpreadsheetChunker(\n",
    "    data=data,\n",
    "    max_chunk_size=1536,\n",
    "    chunking_by_row=True,  # or False for sheet-based chunking\n",
    "    include_header_in_chunks=True\n",
    ")\n",
    "\n",
    "# Get chunks\n",
    "chunks = chunker.get_chunks()\n",
    "\n",
    "# Display results\n",
    "print(f\"✅ Created {len(chunks)} chunks from {excel_file_path}\")\n",
    "for chunk in chunks[:10]:  # Show first 10 chunks\n",
    "    if \"summary\" in chunk:\n",
    "        print(chunk[\"summary\"])\n",
    "    print(f\"\\n📄 Chunk {chunk['chunk_id']}: {chunk['title']}\")\n",
    "    print(chunk['content'][:300] + \"...\" if len(chunk['content']) > 300 else chunk['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355f66c6",
   "metadata": {},
   "source": [
    "## 3. Sheet-Level Chunking with AI Summaries 🤖\n",
    "\n",
    "Instead of creating individual chunks for each row, let's try chunking **by sheet**. This approach:\n",
    "\n",
    "- Creates one chunk per worksheet\n",
    "- Uses AI to generate a summary of the entire sheet\n",
    "- Better for high-level understanding of spreadsheet data\n",
    "- Reduces total number of chunks for large spreadsheets\n",
    "\n",
    "Set `chunking_by_row=False` to enable this mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275224d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chunker with sheet-level chunking (with AI summaries)\n",
    "chunker_by_sheet = SpreadsheetChunker(\n",
    "    data=data,\n",
    "    max_chunk_size=1536,\n",
    "    chunking_by_row=False,  # Chunk by sheet instead of by row\n",
    "    include_header_in_chunks=False  # Not applicable for sheet-level chunking\n",
    ")\n",
    "\n",
    "# Get chunks\n",
    "sheet_chunks = chunker_by_sheet.get_chunks()\n",
    "\n",
    "# Display results\n",
    "print(f\"✅ Created {len(sheet_chunks)} chunks (one per sheet) from {excel_file_path}\")\n",
    "for chunk in sheet_chunks:\n",
    "    print(f\"\\n📄 Chunk {chunk['chunk_id']}: {chunk['title']}\")\n",
    "    print(f\"\\n🤖 AI Summary:\\n{chunk.get('summary', 'No summary available')}\")\n",
    "    print(f\"\\n📊 Table Data (first 500 chars):\\n{chunk['content'][:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9a45aa",
   "metadata": {},
   "source": [
    "## 4. Comparing Chunking Strategies\n",
    "\n",
    "Let's compare the two approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a03357",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📊 Chunking Strategy Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Row-by-row chunking: {len(chunks)} chunks\")\n",
    "print(f\"Sheet-level chunking: {len(sheet_chunks)} chunks\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n💡 Use Cases:\")\n",
    "print(\"\\nRow-by-row:\")\n",
    "print(\"  ✅ Granular search across individual records\")\n",
    "print(\"  ✅ When each row represents a distinct entity\")\n",
    "print(\"  ✅ Better for precise retrieval of specific data points\")\n",
    "print(\"\\nSheet-level:\")\n",
    "print(\"  ✅ High-level understanding of data structure\")\n",
    "print(\"  ✅ When you need summaries of entire datasets\")\n",
    "print(\"  ✅ Reduces number of chunks for large spreadsheets\")\n",
    "print(\"  ✅ Better for understanding overall trends and patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c7277e",
   "metadata": {},
   "source": [
    "## 5. Next Steps: Integrating with RAG\n",
    "\n",
    "Now that you have chunks, you can:\n",
    "\n",
    "1. **Generate embeddings** for each chunk using the embedding model\n",
    "2. **Upload to Azure AI Search** with vector search capability\n",
    "3. **Query the index** to retrieve relevant spreadsheet data\n",
    "4. **Use with chat model** to answer questions about your spreadsheet data\n",
    "\n",
    "See [3-basic-rag.ipynb](3-basic-rag.ipynb) for the complete RAG pipeline implementation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd063230",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "You've learned how to:\n",
    "\n",
    "✅ Load and process local Excel files  \n",
    "✅ Use row-by-row chunking for granular data access  \n",
    "✅ Use sheet-level chunking with AI summaries  \n",
    "✅ Prepare spreadsheet data for RAG pipelines  \n",
    "\n",
    "Choose the chunking strategy that best fits your use case! 🎯"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
