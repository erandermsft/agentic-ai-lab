{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cf62fbc",
   "metadata": {},
   "source": [
    "# üçè Basic Retrieval-Augmented Generation (RAG) with AIProjectClient üçé\n",
    "\n",
    "In this notebook, we'll demonstrate a **basic RAG** flow using:\n",
    "- **`azure-ai-projects`** (AIProjectClient)\n",
    "- **`azure-ai-inference`** (Embeddings, ChatCompletions)\n",
    "- **`azure-ai-search`** (for vector or hybrid search)\n",
    "\n",
    "Our theme is **Health & Fitness** üçè so we‚Äôll create a simple set of health tips, embed them, store them in a search index, then do a query that retrieves relevant tips, and pass them to an LLM to produce a final answer.\n",
    "\n",
    "> **Disclaimer**: This is not medical advice. For real health questions, consult a professional.\n",
    "\n",
    "## What is RAG?\n",
    "Retrieval-Augmented Generation (RAG) is a technique where the LLM (Large Language Model) uses relevant retrieved text chunks from your data to craft a final answer. This helps ground the model's response in real data, reducing hallucinations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26cbaa3",
   "metadata": {},
   "source": [
    "<img src=\"./seq-diagrams/3-basic-rag.png\" width=\"75%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dfbd81",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "We'll import libraries, load environment variables, and create an `AIProjectClient`.\n",
    "\n",
    "> #### Complete [2-embeddings.ipynb](2-embeddings.ipynb) notebook before starting this one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cef07a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7395b2e",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîë Using Tenant ID: 1f7b70e0-e301-49a8-9bb0-70188e795280\n",
      "üîé Using Azure AI Search endpoint: https://aisearch0102.search.windows.net\n",
      "üåê Using browser-based authentication to bypass Azure CLI cache issues...\n",
      "‚úÖ AIProjectClient created successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# azure-ai-projects\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import InteractiveBrowserCredential\n",
    "\n",
    "# We'll embed with azure-ai-inference\n",
    "from azure.ai.inference import EmbeddingsClient, ChatCompletionsClient\n",
    "from azure.ai.inference.models import UserMessage, SystemMessage\n",
    "\n",
    "# For vector search or hybrid search\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "# Load environment variables\n",
    "notebook_path = Path().absolute()\n",
    "parent_dir = notebook_path.parent\n",
    "load_dotenv(parent_dir / '.env')\n",
    "\n",
    "##### Need to add these into your .env file #####\n",
    "project_endpoint = os.environ.get(\"AI_FOUNDRY_PROJECT_ENDPOINT\")\n",
    "chat_model = os.environ.get(\"MODEL_DEPLOYMENT_NAME\", \"gpt-4o-mini\")\n",
    "embedding_model = os.environ.get(\"EMBEDDING_MODEL_DEPLOYMENT_NAME\", \"text-embedding-3-small\")\n",
    "search_index_name = os.environ.get(\"SEARCH_INDEX_NAME\", \"healthtips-index\") + str(random.randint(0,1000))\n",
    "tenant_id = os.environ.get(\"TENANT_ID\")\n",
    "search_endpoint = os.environ.get(\"AZURE_AI_SEARCH_ENDPOINT\")  # New: explicit search endpoint\n",
    "\n",
    "print(f\"üîë Using Tenant ID: {tenant_id}\")\n",
    "print(f\"üîé Using Azure AI Search endpoint: {search_endpoint}\")\n",
    "if not search_endpoint:\n",
    "    raise ValueError(\"‚ùå AZURE_AI_SEARCH_ENDPOINT not set in environment. Please add it to your .env file.\")\n",
    "\n",
    "try:\n",
    "    print(\"üåê Using browser-based authentication to bypass Azure CLI cache issues...\")\n",
    "    \n",
    "    # Use only InteractiveBrowserCredential with the specific tenant\n",
    "    credential = InteractiveBrowserCredential(tenant_id=tenant_id)\n",
    "    \n",
    "    # Create the project client using endpoint\n",
    "    project_client = AIProjectClient(\n",
    "        endpoint=project_endpoint,\n",
    "        credential=credential\n",
    "    )\n",
    "    print(\"‚úÖ AIProjectClient created successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Error creating AIProjectClient:\", e)\n",
    "    print(\"üí° Please complete the browser authentication prompt that should appear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b84bb8",
   "metadata": {},
   "source": [
    "## 2. Create Sample Health Data\n",
    "We'll create a few short doc chunks. In a real scenario, you might read from CSV or PDFs, chunk them up, embed them, and store them in your search index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eab53d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a small list of health tips.\n"
     ]
    }
   ],
   "source": [
    "health_tips = [\n",
    "    {\n",
    "        \"id\": \"doc1\",\n",
    "        \"content\": \"Daily 30-minute walks help maintain a healthy weight and reduce stress.\",\n",
    "        \"source\": \"General Fitness\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc2\",\n",
    "        \"content\": \"Stay hydrated by drinking 8-10 cups of water per day.\",\n",
    "        \"source\": \"General Fitness\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc3\",\n",
    "        \"content\": \"Consistent sleep patterns (7-9 hours) improve muscle recovery.\",\n",
    "        \"source\": \"General Fitness\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc4\",\n",
    "        \"content\": \"For cardio endurance, try interval training like HIIT.\",\n",
    "        \"source\": \"Workout Advice\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc5\",\n",
    "        \"content\": \"Warm up with dynamic stretches before running to reduce injury risk.\",\n",
    "        \"source\": \"Workout Advice\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc6\",\n",
    "        \"content\": \"Balanced diets typically include protein, whole grains, fruits, vegetables, and healthy fats.\",\n",
    "        \"source\": \"Nutrition\"\n",
    "    },\n",
    "]\n",
    "print(\"Created a small list of health tips.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55b8d39",
   "metadata": {},
   "source": [
    "## 3.0. Create or Reset the Index\n",
    "When creating a vector field in Azure AI Search, the **field definition** must include a `vector_search_profile` property that points to a matching profile name in your vector search settings.\n",
    "\n",
    "We'll define a helper function to create (or reset) a vector index with an HNSW algorithm config.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabd84b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    HnswParameters,\n",
    "    VectorSearchAlgorithmKind,\n",
    "    VectorSearchAlgorithmMetric,\n",
    "    VectorSearchProfile,\n",
    "    )\n",
    "\n",
    "def create_healthtips_index(\n",
    "        endpoint: str,\n",
    "        index_name: str, \n",
    "        dimension: int = 1536, # if using text-embedding-3-small\n",
    "        credential=None\n",
    "        ):\n",
    "    \"\"\"Create or update a search index for health tips with vector search capability.\"\"\"\n",
    "    \n",
    "    index_client = SearchIndexClient(endpoint=endpoint, credential=credential)\n",
    "    \n",
    "    # Try to delete existing index\n",
    "    try:\n",
    "        index_client.delete_index(index_name)\n",
    "        print(f\"Deleted existing index: {index_name}\")\n",
    "    except Exception:\n",
    "        pass  # Index doesn't exist yet\n",
    "        \n",
    "    # Define vector search configuration\n",
    "    vector_search = VectorSearch(\n",
    "        algorithms=[\n",
    "            HnswAlgorithmConfiguration(\n",
    "                name=\"myHnsw\",\n",
    "                kind=VectorSearchAlgorithmKind.HNSW,\n",
    "                parameters=HnswParameters(\n",
    "                    m=4,\n",
    "                    ef_construction=400,\n",
    "                    ef_search=500,\n",
    "                    metric=VectorSearchAlgorithmMetric.COSINE\n",
    "                )\n",
    "            )\n",
    "        ],\n",
    "        profiles=[\n",
    "            VectorSearchProfile(\n",
    "                name=\"myHnswProfile\",\n",
    "                algorithm_configuration_name=\"myHnsw\"\n",
    "            )\n",
    "        ]\n",
    ")\n",
    "    \n",
    "    # Define fields\n",
    "    fields = [\n",
    "        SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "        SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "        SimpleField(name=\"source\", type=SearchFieldDataType.String),\n",
    "        SearchField(\n",
    "            name=\"embedding\", \n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            vector_search_dimensions=dimension,\n",
    "            vector_search_profile_name=\"myHnswProfile\" \n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    # Create index definition\n",
    "    index_def = SearchIndex(\n",
    "        name=index_name,\n",
    "        fields=fields,\n",
    "        vector_search=vector_search\n",
    "    )\n",
    "    \n",
    "    # Create the index\n",
    "    index_client.create_index(index_def)\n",
    "    print(f\"‚úÖ Created or reset index: {index_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b4d51a",
   "metadata": {},
   "source": [
    "## 3.1. Create Index & Upload Health Tips üèãÔ∏è\n",
    "\n",
    "Now we'll put our health tips into action by:\n",
    "1. **Creating a search connection** to Azure AI Search\n",
    "2. **Building our index** with vector search capability\n",
    "3. **Generating embeddings** for each health tip\n",
    "4. **Uploading** the tips with their embeddings\n",
    "\n",
    "This creates our knowledge base that we'll search through later. Think of it as building our 'fitness library' that our AI assistant can reference! üìöüí™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c917b7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check the search index name\n",
    "\n",
    "if not search_index_name:\n",
    "    print(\"‚ùå search_index_name is empty or None, setting default...\")\n",
    "    search_index_name = \"healthtips-index\"\n",
    "    print(f\"‚úÖ Set search_index_name to: '{search_index_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f2f690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using Azure AI Search endpoint: https://aisearch0102.search.windows.net\n",
      "‚úÖ Got embedding length: 1536\n",
      "Deleted existing index: healthtips-index59\n",
      "‚úÖ Created or reset index: healthtips-index59\n",
      "‚úÖ Created search client\n"
     ]
    },
    {
     "ename": "HttpResponseError",
     "evalue": "Operation returned an invalid status 'Forbidden'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHttpResponseError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 54\u001b[0m\n\u001b[0;32m     46\u001b[0m         search_docs\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     47\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     48\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     49\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     50\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m: emb_vec,\n\u001b[0;32m     51\u001b[0m         })\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Upload documents to index\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m result \u001b[38;5;241m=\u001b[39m search_client\u001b[38;5;241m.\u001b[39mupload_documents(documents\u001b[38;5;241m=\u001b[39msearch_docs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Uploaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(search_docs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m documents to search index \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msearch_index_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\albinlnnflt\\AppData\\Local\\anaconda3\\envs\\playground\\Lib\\site-packages\\azure\\search\\documents\\_search_client.py:578\u001b[0m, in \u001b[0;36mSearchClient.upload_documents\u001b[1;34m(self, documents, **kwargs)\u001b[0m\n\u001b[0;32m    575\u001b[0m batch\u001b[38;5;241m.\u001b[39madd_upload_actions(documents)\n\u001b[0;32m    577\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_client_headers(kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m--> 578\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_documents(batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(List[IndexingResult], results)\n",
      "File \u001b[1;32mc:\\Users\\albinlnnflt\\AppData\\Local\\anaconda3\\envs\\playground\\Lib\\site-packages\\azure\\core\\tracing\\decorator.py:138\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m span_attributes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    137\u001b[0m                 span\u001b[38;5;241m.\u001b[39madd_attribute(key, value)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m# Native path\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\albinlnnflt\\AppData\\Local\\anaconda3\\envs\\playground\\Lib\\site-packages\\azure\\search\\documents\\_search_client.py:685\u001b[0m, in \u001b[0;36mSearchClient.index_documents\u001b[1;34m(self, batch, **kwargs)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;129m@distributed_trace\u001b[39m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mindex_documents\u001b[39m(\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28mself\u001b[39m, batch: IndexDocumentsBatch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[0;32m    675\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[IndexingResult]:\n\u001b[0;32m    676\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Specify a document operations to perform as a batch.\u001b[39;00m\n\u001b[0;32m    677\u001b[0m \n\u001b[0;32m    678\u001b[0m \u001b[38;5;124;03m    :param batch: A batch of document operations to perform.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;124;03m    :raises ~azure.search.documents.RequestEntityTooLargeError\u001b[39;00m\n\u001b[0;32m    684\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 685\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_documents_actions(actions\u001b[38;5;241m=\u001b[39mbatch\u001b[38;5;241m.\u001b[39mactions, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\albinlnnflt\\AppData\\Local\\anaconda3\\envs\\playground\\Lib\\site-packages\\azure\\search\\documents\\_search_client.py:695\u001b[0m, in \u001b[0;36mSearchClient._index_documents_actions\u001b[1;34m(self, actions, **kwargs)\u001b[0m\n\u001b[0;32m    693\u001b[0m batch \u001b[38;5;241m=\u001b[39m IndexBatch(actions\u001b[38;5;241m=\u001b[39mactions)\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 695\u001b[0m     batch_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mdocuments\u001b[38;5;241m.\u001b[39mindex(\n\u001b[0;32m    696\u001b[0m         batch\u001b[38;5;241m=\u001b[39mbatch, error_map\u001b[38;5;241m=\u001b[39merror_map, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    697\u001b[0m     )\n\u001b[0;32m    698\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(List[IndexingResult], batch_response\u001b[38;5;241m.\u001b[39mresults)\n\u001b[0;32m    699\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RequestEntityTooLargeError:\n",
      "File \u001b[1;32mc:\\Users\\albinlnnflt\\AppData\\Local\\anaconda3\\envs\\playground\\Lib\\site-packages\\azure\\core\\tracing\\decorator.py:138\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m span_attributes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    137\u001b[0m                 span\u001b[38;5;241m.\u001b[39madd_attribute(key, value)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m# Native path\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\albinlnnflt\\AppData\\Local\\anaconda3\\envs\\playground\\Lib\\site-packages\\azure\\search\\documents\\_generated\\operations\\_documents_operations.py:1455\u001b[0m, in \u001b[0;36mDocumentsOperations.index\u001b[1;34m(self, batch, request_options, **kwargs)\u001b[0m\n\u001b[0;32m   1449\u001b[0m     map_error(\n\u001b[0;32m   1450\u001b[0m         status_code\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code, response\u001b[38;5;241m=\u001b[39mresponse, error_map\u001b[38;5;241m=\u001b[39merror_map\n\u001b[0;32m   1451\u001b[0m     )\n\u001b[0;32m   1452\u001b[0m     error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deserialize\u001b[38;5;241m.\u001b[39mfailsafe_deserialize(\n\u001b[0;32m   1453\u001b[0m         _models\u001b[38;5;241m.\u001b[39mErrorResponse, pipeline_response\n\u001b[0;32m   1454\u001b[0m     )\n\u001b[1;32m-> 1455\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response\u001b[38;5;241m=\u001b[39mresponse, model\u001b[38;5;241m=\u001b[39merror)\n\u001b[0;32m   1457\u001b[0m deserialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deserialize(\n\u001b[0;32m   1458\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndexDocumentsResult\u001b[39m\u001b[38;5;124m\"\u001b[39m, pipeline_response\u001b[38;5;241m.\u001b[39mhttp_response\n\u001b[0;32m   1459\u001b[0m )\n\u001b[0;32m   1461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m:\n",
      "\u001b[1;31mHttpResponseError\u001b[0m: Operation returned an invalid status 'Forbidden'"
     ]
    }
   ],
   "source": [
    "# 3.1. Create Index & Upload Health Tips using explicit endpoint + AAD auth\n",
    "\n",
    "# Step 1: Validate we have an endpoint\n",
    "if not search_endpoint:\n",
    "    raise RuntimeError(\"‚ùå AZURE_AI_SEARCH_ENDPOINT not found. Please set it in your .env file.\")\n",
    "print(f\"‚úÖ Using Azure AI Search endpoint: {search_endpoint}\")\n",
    "\n",
    "# Step 2: Determine embedding length dynamically\n",
    "with project_client.get_openai_client(api_version=\"2024-10-21\") as embeddings_client:\n",
    "    sample_doc = health_tips[0]\n",
    "    emb_response = embeddings_client.embeddings.create(\n",
    "        model=embedding_model,\n",
    "        input=[sample_doc[\"content\"]]\n",
    "    )\n",
    "    embedding_length = len(emb_response.data[0].embedding)\n",
    "    print(f\"‚úÖ Got embedding length: {embedding_length}\")\n",
    "\n",
    "# Step 3: Create the index\n",
    "create_healthtips_index(\n",
    "    endpoint=search_endpoint,\n",
    "    credential=credential,  # InteractiveBrowserCredential from earlier\n",
    "    index_name=search_index_name,\n",
    "    dimension=embedding_length\n",
    ")\n",
    "\n",
    "# Step 4: Create search client for uploading documents\n",
    "search_client = SearchClient(\n",
    "    endpoint=search_endpoint,\n",
    "    index_name=search_index_name,\n",
    "    credential=credential,\n",
    ")\n",
    "print(\"‚úÖ Created search client\")\n",
    "\n",
    "# Step 5: Embed and upload documents\n",
    "search_docs = []\n",
    "with project_client.get_openai_client(api_version=\"2024-10-21\") as embed_client:\n",
    "    for doc in health_tips:\n",
    "        # Get embedding for document content\n",
    "        emb_response = embed_client.embeddings.create(\n",
    "            model=embedding_model,\n",
    "            input=[doc[\"content\"]]\n",
    "        )\n",
    "        emb_vec = emb_response.data[0].embedding\n",
    "\n",
    "        # Create document with embedding\n",
    "        search_docs.append({\n",
    "            \"id\": doc[\"id\"],\n",
    "            \"content\": doc[\"content\"],\n",
    "            \"source\": doc[\"source\"],\n",
    "            \"embedding\": emb_vec,\n",
    "        })\n",
    "\n",
    "# Upload documents to index\n",
    "result = search_client.upload_documents(documents=search_docs)\n",
    "print(f\"‚úÖ Uploaded {len(search_docs)} documents to search index '{search_index_name}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05e5468",
   "metadata": {},
   "source": [
    "## 4. Basic RAG Flow\n",
    "### 4.1. Retrieve\n",
    "When a user queries, we:\n",
    "1. Embed user question.\n",
    "2. Search vector index with that embedding to get top docs.\n",
    "\n",
    "### 4.2. Generate answer\n",
    "We then pass the retrieved docs to the chat model.\n",
    "\n",
    "> In a real scenario, you'd have a more advanced approach to chunking & summarizing. We'll keep it simple.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c15c3aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.models import VectorizedQuery\n",
    "\n",
    "def rag_chat(query: str, top_k: int = 3) -> str:\n",
    "    # 1) Embed user query\n",
    "    with project_client.get_openai_client(api_version=\"2024-10-21\") as embed_client:\n",
    "        user_vec_response = embed_client.embeddings.create(\n",
    "            model=embedding_model,\n",
    "            input=[query]\n",
    "        )\n",
    "        user_vec = user_vec_response.data[0].embedding\n",
    "\n",
    "    # 2) Vector search using VectorizedQuery\n",
    "    vector_query = VectorizedQuery(\n",
    "        vector=user_vec,\n",
    "        k_nearest_neighbors=top_k,\n",
    "        fields=\"embedding\"\n",
    "    )\n",
    "\n",
    "    results = search_client.search(\n",
    "        search_text=\"\",  # Optional text query\n",
    "        vector_queries=[vector_query],\n",
    "        select=[\"content\", \"source\"]  # Only retrieve fields we need\n",
    "    )\n",
    "\n",
    "    # gather the top docs\n",
    "    top_docs_content = []\n",
    "    for r in results:\n",
    "        c = r[\"content\"]\n",
    "        s = r[\"source\"]\n",
    "        top_docs_content.append(f\"Source: {s} => {c}\")\n",
    "\n",
    "    # 3) Chat with retrieved docs\n",
    "    system_text = (\n",
    "        \"You are a health & fitness assistant.\\n\"\n",
    "        \"Answer user questions using ONLY the text from these docs.\\n\"\n",
    "        \"Docs:\\n\"\n",
    "        + \"\\n\".join(top_docs_content)\n",
    "        + \"\\nIf unsure, say 'I'm not sure'.\\n\"\n",
    "    )\n",
    "\n",
    "    with project_client.get_openai_client(api_version=\"2024-10-21\") as chat_client:\n",
    "        response = chat_client.chat.completions.create(\n",
    "            model=chat_model,\n",
    "            messages=[\n",
    "                SystemMessage(content=system_text),\n",
    "                UserMessage(content=query)\n",
    "            ]\n",
    "        )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfecfb3c",
   "metadata": {},
   "source": [
    "## 5. Try a Query üéâ\n",
    "Let's do a question about cardio for busy people.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3937fdfc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'search_client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m user_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms a good short cardio routine for me if I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm busy?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m answer \u001b[38;5;241m=\u001b[39m rag_chat(user_query)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müó£Ô∏è User Query:\u001b[39m\u001b[38;5;124m\"\u001b[39m, user_query)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mü§ñ RAG Answer:\u001b[39m\u001b[38;5;124m\"\u001b[39m, answer)\n",
      "Cell \u001b[1;32mIn[6], line 19\u001b[0m, in \u001b[0;36mrag_chat\u001b[1;34m(query, top_k)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# 2) Vector search using VectorizedQuery\u001b[39;00m\n\u001b[0;32m     13\u001b[0m vector_query \u001b[38;5;241m=\u001b[39m VectorizedQuery(\n\u001b[0;32m     14\u001b[0m     vector\u001b[38;5;241m=\u001b[39muser_vec,\n\u001b[0;32m     15\u001b[0m     k_nearest_neighbors\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m     16\u001b[0m     fields\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     17\u001b[0m )\n\u001b[1;32m---> 19\u001b[0m results \u001b[38;5;241m=\u001b[39m search_client\u001b[38;5;241m.\u001b[39msearch(\n\u001b[0;32m     20\u001b[0m     search_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Optional text query\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     vector_queries\u001b[38;5;241m=\u001b[39m[vector_query],\n\u001b[0;32m     22\u001b[0m     select\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# Only retrieve fields we need\u001b[39;00m\n\u001b[0;32m     23\u001b[0m )\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# gather the top docs\u001b[39;00m\n\u001b[0;32m     26\u001b[0m top_docs_content \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'search_client' is not defined"
     ]
    }
   ],
   "source": [
    "user_query = \"What's a good short cardio routine for me if I'm busy?\"\n",
    "answer = rag_chat(user_query)\n",
    "print(\"üó£Ô∏è User Query:\", user_query)\n",
    "print(\"ü§ñ RAG Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e562e6",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "We've demonstrated a **basic RAG** pipeline with:\n",
    "- **Embedding** docs & storing them in **Azure AI Search**.\n",
    "- **Retrieving** top docs for user question.\n",
    "- **Chat** with the retrieved docs.\n",
    "\n",
    "üîé You can expand this by adding advanced chunking, more robust retrieval, and quality checks. Enjoy your healthy coding! üçé"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
