{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abc8909d",
   "metadata": {},
   "source": [
    "# Azure AI Agent Basic Example\n",
    "\n",
    "This notebook demonstrates basic usage of AzureAIAgentClient to create agents with automatic lifecycle management. It shows both streaming and non-streaming responses with function tools.\n",
    "\n",
    "## Features Covered:\n",
    "- Creating an Azure AI Agent with automatic lifecycle management\n",
    "- Using function tools (weather function)\n",
    "- Non-streaming responses (get complete result at once)\n",
    "- Streaming responses (get results as they are generated)\n",
    "- Authentication using Azure CLI credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eb53f4",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, make sure you have:\n",
    "1. Installed the agent-framework packages\n",
    "2. Authenticated with Azure CLI (`az login --use-device-code`)\n",
    "3. Configured your Azure AI services"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da47fa9b",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "First, let's import all the necessary libraries and modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c87dfd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft. All rights reserved.\n",
    "\n",
    "import asyncio\n",
    "from random import randint\n",
    "from typing import Annotated\n",
    "\n",
    "from agent_framework.azure import AzureAIAgentClient\n",
    "from azure.identity.aio import AzureCliCredential\n",
    "from pydantic import Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb95fd4",
   "metadata": {},
   "source": [
    "## Initial Setup\n",
    "We'll start by importing needed libraries, loading environment variables, and initializing an **AIProjectClient** so we can do all the agent-related actions. Let's do it! 🎉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37198f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔑 Using Tenant ID: 38271ae3-334d-4c80-9c44-d642253759fa\n",
      "🌐 Using browser-based authentication to bypass Azure CLI cache issues...\n",
      "✅ Successfully initialized AIProjectClient\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path  # For working with file paths\n",
    "import os  # For environment variables\n",
    "import time  # For sleep function\n",
    "from dotenv import load_dotenv  # For loading environment variables from .env file\n",
    "from azure.identity import InteractiveBrowserCredential  # For Azure authentication\n",
    "from azure.ai.projects import AIProjectClient  # Import the AIProjectClient\n",
    "\n",
    "# Get the path to the .env file which is in the parent directory\n",
    "notebook_path = Path().absolute()  # Get absolute path of current notebook\n",
    "parent_dir = notebook_path.parent  # Get parent directory\n",
    "load_dotenv('../../../.env')  # Load environment variables from .env file\n",
    "\n",
    "# Get tenant ID for authentication\n",
    "tenant_id = os.environ.get(\"TENANT_ID\")\n",
    "conn_string = os.environ.get(\"PROJECT_CONNECTION_STRING\")\n",
    "\n",
    "print(f\"🔑 Using Tenant ID: {tenant_id}\")\n",
    "\n",
    "# Initialize the AI Project Client using simplified browser-based authentication\n",
    "try:\n",
    "    print(\"🌐 Using browser-based authentication to bypass Azure CLI cache issues...\")\n",
    "    \n",
    "    # Use only InteractiveBrowserCredential with the specific tenant\n",
    "    credential = InteractiveBrowserCredential(tenant_id=tenant_id)\n",
    "    \n",
    "    # Create the project client using the endpoint\n",
    "    project_client = AIProjectClient(\n",
    "        endpoint=conn_string,\n",
    "        credential=credential\n",
    "    )\n",
    "    print(\"✅ Successfully initialized AIProjectClient\")\n",
    "except Exception as e:\n",
    "    # Print error message if client initialization fails\n",
    "    print(f\"❌ Error initializing project client: {str(e)}\")\n",
    "    print(\"💡 Please complete the browser authentication prompt that should appear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6bb514",
   "metadata": {},
   "source": [
    "## Define Function Tools\n",
    "\n",
    "Function tools allow the agent to call specific functions to gather information or perform actions. Here we define a simple weather function that the agent can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b074eaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(\n",
    "    location: Annotated[str, Field(description=\"The location to get the weather for.\")],\n",
    ") -> str:\n",
    "    \"\"\"Get the weather for a given location.\"\"\"\n",
    "    conditions = [\"sunny\", \"cloudy\", \"rainy\", \"stormy\"]\n",
    "    return f\"The weather in {location} is {conditions[randint(0, 3)]} with a high of {randint(10, 30)}°C.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d1a9e8",
   "metadata": {},
   "source": [
    "## Non-Streaming Response Example\n",
    "\n",
    "In this example, we'll create an agent and get a complete response at once (non-streaming). The agent will be automatically created and deleted after getting the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a90633",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def non_streaming_example() -> None:\n",
    "    \"\"\"Example of non-streaming response (get the complete result at once).\"\"\"\n",
    "    print(\"=== Non-streaming Response Example ===\")\n",
    "\n",
    "    # Use the agent framework with proper initialization\n",
    "    # The issue is that we need to use the endpoint directly, not the project_client\n",
    "    async with (\n",
    "        AzureCliCredential() as credential,\n",
    "        AzureAIAgentClient(\n",
    "            project_endpoint=conn_string,  # Use project_endpoint instead of project_client\n",
    "            async_credential=credential,\n",
    "            model_deployment_name=os.environ.get(\"AZURE_AI_MODEL_DEPLOYMENT_NAME\")  # Specify the model deployment name\n",
    "        ).create_agent(\n",
    "            name=\"WeatherAgent\",\n",
    "            instructions=\"You are a helpful weather agent.\",\n",
    "            tools=get_weather,\n",
    "        ) as agent,\n",
    "    ):\n",
    "        query = \"What's the weather like in Seattle?\"\n",
    "        print(f\"User: {query}\")\n",
    "        result = await agent.run(query)\n",
    "        print(f\"Agent: {result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d857ca",
   "metadata": {},
   "source": [
    "## Streaming Response Example\n",
    "\n",
    "In this example, we'll demonstrate streaming responses where we get results as they are generated by the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "019a53b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def streaming_example() -> None:\n",
    "    \"\"\"Example of streaming response (get results as they are generated).\"\"\"\n",
    "    print(\"=== Streaming Response Example ===\")\n",
    "\n",
    "    # Use the agent framework with proper initialization\n",
    "    async with (\n",
    "        AzureCliCredential() as credential,\n",
    "        AzureAIAgentClient(\n",
    "            project_endpoint=conn_string,  # Use project_endpoint instead of project_client\n",
    "            async_credential=credential,\n",
    "            model_deployment_name=os.environ.get(\"AZURE_AI_MODEL_DEPLOYMENT_NAME\")  # Specify the model deployment name\n",
    "        ).create_agent(\n",
    "            name=\"WeatherAgent\",\n",
    "            instructions=\"You are a helpful weather agent.\",\n",
    "            tools=get_weather,\n",
    "        ) as agent,\n",
    "    ):\n",
    "        query = \"What's the weather like in Portland?\"\n",
    "        print(f\"User: {query}\")\n",
    "        print(\"Agent: \", end=\"\", flush=True)\n",
    "        async for chunk in agent.run_stream(query):\n",
    "            if chunk.text:\n",
    "                print(chunk.text, end=\"\", flush=True)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb42e74",
   "metadata": {},
   "source": [
    "## Main Execution Function\n",
    "\n",
    "This function orchestrates the execution of both examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90b61963",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main() -> None:\n",
    "    print(\"=== Basic Azure AI Chat Client Agent Example ===\")\n",
    "\n",
    "    await non_streaming_example()\n",
    "    await streaming_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1225c4c",
   "metadata": {},
   "source": [
    "## Run the Examples\n",
    "\n",
    "Execute the main function to run both streaming and non-streaming examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e9f76b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Basic Azure AI Chat Client Agent Example ===\n",
      "=== Non-streaming Response Example ===\n",
      "User: What's the weather like in Seattle?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-29 21:36:06 - c:\\Users\\erander\\source\\erandermsft\\agentic-ai-lab\\.venv\\Lib\\site-packages\\agent_framework_azure_ai\\_chat_client.py:677 - ERROR] Error processing stream: Failed to resolve model info for: gpt-4o\n"
     ]
    },
    {
     "ename": "ServiceResponseException",
     "evalue": "Failed to resolve model info for: gpt-4o",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mServiceResponseException\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run the main function\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m main()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m() -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m      2\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== Basic Azure AI Chat Client Agent Example ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m non_streaming_example()\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m streaming_example()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mnon_streaming_example\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     19\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mWhat\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms the weather like in Seattle?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUser: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m agent.run(query)\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAgent: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erander\\source\\erandermsft\\agentic-ai-lab\\.venv\\Lib\\site-packages\\agent_framework\\_middleware.py:1249\u001b[39m, in \u001b[36muse_agent_middleware.<locals>.middleware_enabled_run\u001b[39m\u001b[34m(self, messages, thread, middleware, **kwargs)\u001b[39m\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;28;01melse\u001b[39;00m AgentRunResponse()\n\u001b[32m   1248\u001b[39m \u001b[38;5;66;03m# No middleware, execute directly\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m original_run(\u001b[38;5;28mself\u001b[39m, normalized_messages, thread=thread, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erander\\source\\erandermsft\\agentic-ai-lab\\.venv\\Lib\\site-packages\\agent_framework\\observability.py:1097\u001b[39m, in \u001b[36m_trace_agent_run.<locals>.trace_run\u001b[39m\u001b[34m(self, messages, thread, **kwargs)\u001b[39m\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01mglobal\u001b[39;00m OBSERVABILITY_SETTINGS\n\u001b[32m   1095\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m OBSERVABILITY_SETTINGS.ENABLED:\n\u001b[32m   1096\u001b[39m     \u001b[38;5;66;03m# If model diagnostics are not enabled, just return the completion\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1097\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m run_func(\u001b[38;5;28mself\u001b[39m, messages=messages, thread=thread, **kwargs)\n\u001b[32m   1098\u001b[39m attributes = _get_span_attributes(\n\u001b[32m   1099\u001b[39m     operation_name=OtelAttr.AGENT_INVOKE_OPERATION,\n\u001b[32m   1100\u001b[39m     provider_name=provider_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1106\u001b[39m     **kwargs,\n\u001b[32m   1107\u001b[39m )\n\u001b[32m   1108\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _get_span(attributes=attributes, span_name_attribute=OtelAttr.AGENT_NAME) \u001b[38;5;28;01mas\u001b[39;00m span:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erander\\source\\erandermsft\\agentic-ai-lab\\.venv\\Lib\\site-packages\\agent_framework\\_agents.py:863\u001b[39m, in \u001b[36mChatAgent.run\u001b[39m\u001b[34m(self, messages, thread, frequency_penalty, logit_bias, max_tokens, metadata, model_id, presence_penalty, response_format, seed, stop, store, temperature, tool_choice, tools, top_p, user, additional_chat_options, **kwargs)\u001b[39m\n\u001b[32m    842\u001b[39m     final_tools.extend(mcp_server.functions)\n\u001b[32m    844\u001b[39m co = run_chat_options & ChatOptions(\n\u001b[32m    845\u001b[39m     model_id=model_id,\n\u001b[32m    846\u001b[39m     conversation_id=thread.service_thread_id,\n\u001b[32m   (...)\u001b[39m\u001b[32m    861\u001b[39m     **(additional_chat_options \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[32m    862\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m863\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chat_client.get_response(messages=thread_messages, chat_options=co, **kwargs)\n\u001b[32m    865\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._update_thread_with_type_and_conversation_id(thread, response.conversation_id)\n\u001b[32m    867\u001b[39m \u001b[38;5;66;03m# Ensure that the author name is set for each message in the response.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erander\\source\\erandermsft\\agentic-ai-lab\\.venv\\Lib\\site-packages\\agent_framework\\_tools.py:1364\u001b[39m, in \u001b[36m_handle_function_calls_response.<locals>.decorator.<locals>.function_invocation_wrapper\u001b[39m\u001b[34m(self, messages, **kwargs)\u001b[39m\n\u001b[32m   1355\u001b[39m         approved_function_results = \u001b[38;5;28;01mawait\u001b[39;00m _execute_function_calls(\n\u001b[32m   1356\u001b[39m             custom_args=kwargs,\n\u001b[32m   1357\u001b[39m             attempt_idx=attempt_idx,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1360\u001b[39m             middleware_pipeline=stored_middleware_pipeline,\n\u001b[32m   1361\u001b[39m         )\n\u001b[32m   1362\u001b[39m     _replace_approval_contents_with_results(prepped_messages, fcc_todo, approved_function_results)\n\u001b[32m-> \u001b[39m\u001b[32m1364\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, messages=prepped_messages, **kwargs)\n\u001b[32m   1365\u001b[39m \u001b[38;5;66;03m# if there are function calls, we will handle them first\u001b[39;00m\n\u001b[32m   1366\u001b[39m function_results = {\n\u001b[32m   1367\u001b[39m     it.call_id \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m response.messages[\u001b[32m0\u001b[39m].contents \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(it, FunctionResultContent)\n\u001b[32m   1368\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erander\\source\\erandermsft\\agentic-ai-lab\\.venv\\Lib\\site-packages\\agent_framework\\observability.py:829\u001b[39m, in \u001b[36m_trace_get_response.<locals>.decorator.<locals>.trace_get_response\u001b[39m\u001b[34m(self, messages, **kwargs)\u001b[39m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mglobal\u001b[39;00m OBSERVABILITY_SETTINGS\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m OBSERVABILITY_SETTINGS.ENABLED:\n\u001b[32m    828\u001b[39m     \u001b[38;5;66;03m# If model_id diagnostics are not enabled, just return the completion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(\n\u001b[32m    830\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    831\u001b[39m         messages=messages,\n\u001b[32m    832\u001b[39m         **kwargs,\n\u001b[32m    833\u001b[39m     )\n\u001b[32m    834\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mtoken_usage_histogram\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.additional_properties:\n\u001b[32m    835\u001b[39m     \u001b[38;5;28mself\u001b[39m.additional_properties[\u001b[33m\"\u001b[39m\u001b[33mtoken_usage_histogram\u001b[39m\u001b[33m\"\u001b[39m] = _get_token_usage_histogram()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erander\\source\\erandermsft\\agentic-ai-lab\\.venv\\Lib\\site-packages\\agent_framework\\_middleware.py:1367\u001b[39m, in \u001b[36muse_chat_middleware.<locals>.middleware_enabled_get_response\u001b[39m\u001b[34m(self, messages, **kwargs)\u001b[39m\n\u001b[32m   1365\u001b[39m \u001b[38;5;66;03m# If no chat middleware, use original method\u001b[39;00m\n\u001b[32m   1366\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chat_middleware_list:\n\u001b[32m-> \u001b[39m\u001b[32m1367\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m original_get_response(\u001b[38;5;28mself\u001b[39m, messages, **kwargs)\n\u001b[32m   1369\u001b[39m \u001b[38;5;66;03m# Create pipeline and execute with middleware\u001b[39;00m\n\u001b[32m   1370\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_types\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOptions\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erander\\source\\erandermsft\\agentic-ai-lab\\.venv\\Lib\\site-packages\\agent_framework\\_clients.py:619\u001b[39m, in \u001b[36mBaseChatClient.get_response\u001b[39m\u001b[34m(self, messages, frequency_penalty, logit_bias, max_tokens, metadata, model_id, presence_penalty, response_format, seed, stop, store, temperature, tool_choice, tools, top_p, user, additional_properties, **kwargs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28mself\u001b[39m._prepare_tool_choice(chat_options=chat_options)\n\u001b[32m    618\u001b[39m filtered_kwargs = \u001b[38;5;28mself\u001b[39m._filter_internal_kwargs(kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inner_get_response(messages=prepped_messages, chat_options=chat_options, **filtered_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erander\\source\\erandermsft\\agentic-ai-lab\\.venv\\Lib\\site-packages\\agent_framework_azure_ai\\_chat_client.py:321\u001b[39m, in \u001b[36mAzureAIAgentClient._inner_get_response\u001b[39m\u001b[34m(self, messages, chat_options, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_inner_get_response\u001b[39m(\n\u001b[32m    315\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    316\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    319\u001b[39m     **kwargs: Any,\n\u001b[32m    320\u001b[39m ) -> ChatResponse:\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ChatResponse.from_chat_response_generator(\n\u001b[32m    322\u001b[39m         updates=\u001b[38;5;28mself\u001b[39m._inner_get_streaming_response(messages=messages, chat_options=chat_options, **kwargs),\n\u001b[32m    323\u001b[39m         output_format_type=chat_options.response_format,\n\u001b[32m    324\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erander\\source\\erandermsft\\agentic-ai-lab\\.venv\\Lib\\site-packages\\agent_framework\\_types.py:2431\u001b[39m, in \u001b[36mChatResponse.from_chat_response_generator\u001b[39m\u001b[34m(cls, updates, output_format_type)\u001b[39m\n\u001b[32m   2411\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Joins multiple updates into a single ChatResponse.\u001b[39;00m\n\u001b[32m   2412\u001b[39m \n\u001b[32m   2413\u001b[39m \u001b[33;03mExample:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2428\u001b[39m \u001b[33;03m    output_format_type: Optional Pydantic model type to parse the response text into structured data.\u001b[39;00m\n\u001b[32m   2429\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2430\u001b[39m msg = \u001b[38;5;28mcls\u001b[39m(messages=[])\n\u001b[32m-> \u001b[39m\u001b[32m2431\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m update \u001b[38;5;129;01min\u001b[39;00m updates:\n\u001b[32m   2432\u001b[39m     _process_update(msg, update)\n\u001b[32m   2433\u001b[39m _finalize_response(msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erander\\source\\erandermsft\\agentic-ai-lab\\.venv\\Lib\\site-packages\\agent_framework_azure_ai\\_chat_client.py:350\u001b[39m, in \u001b[36mAzureAIAgentClient._inner_get_streaming_response\u001b[39m\u001b[34m(self, messages, chat_options, **kwargs)\u001b[39m\n\u001b[32m    347\u001b[39m agent_id = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_agent_id_or_create(run_options)\n\u001b[32m    349\u001b[39m \u001b[38;5;66;03m# Process and yield each update from the stream\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m update \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_stream(\n\u001b[32m    351\u001b[39m     *(\u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_agent_stream(thread_id, agent_id, run_options, required_action_results))\n\u001b[32m    352\u001b[39m ):\n\u001b[32m    353\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m update\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erander\\source\\erandermsft\\agentic-ai-lab\\.venv\\Lib\\site-packages\\agent_framework_azure_ai\\_chat_client.py:581\u001b[39m, in \u001b[36mAzureAIAgentClient._process_stream\u001b[39m\u001b[34m(self, stream, thread_id)\u001b[39m\n\u001b[32m    572\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m ChatResponseUpdate(\n\u001b[32m    573\u001b[39m                 role=Role.ASSISTANT,\n\u001b[32m    574\u001b[39m                 contents=function_call_contents,\n\u001b[32m   (...)\u001b[39m\u001b[32m    578\u001b[39m                 response_id=response_id,\n\u001b[32m    579\u001b[39m             )\n\u001b[32m    580\u001b[39m \u001b[38;5;28;01mcase\u001b[39;00m AgentStreamEvent.THREAD_RUN_FAILED:\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceResponseException(event_data.last_error.message)\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01mcase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01m_\u001b[39;00m:\n\u001b[32m    583\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m ChatResponseUpdate(\n\u001b[32m    584\u001b[39m         contents=[],\n\u001b[32m    585\u001b[39m         conversation_id=event_data.thread_id,\n\u001b[32m   (...)\u001b[39m\u001b[32m    590\u001b[39m         model_id=event_data.model,\n\u001b[32m    591\u001b[39m     )\n",
      "\u001b[31mServiceResponseException\u001b[39m: Failed to resolve model info for: gpt-4o"
     ]
    }
   ],
   "source": [
    "# Run the main function\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369e3849",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Automatic Lifecycle Management**: When no Agent ID is provided, the agent is automatically created and cleaned up\n",
    "2. **Function Tools**: Agents can use custom functions to perform specific tasks (like getting weather data)\n",
    "3. **Authentication**: Uses Azure CLI credentials for authentication (make sure to run `az login` first)\n",
    "4. **Response Types**: \n",
    "   - Non-streaming: Get complete response at once using `agent.run()`\n",
    "   - Streaming: Get response chunks as they're generated using `agent.run_stream()`\n",
    "5. **Context Managers**: Using `async with` ensures proper resource cleanup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
