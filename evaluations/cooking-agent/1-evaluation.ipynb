{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b04245fc",
   "metadata": {},
   "source": [
    "# 🔬 Lab: Evaluating the Cooking AI Agent\n",
    "\n",
    "## Overview\n",
    "\n",
    "This lab walks you through the complete process of evaluating an AI agent using **Azure AI Foundry's cloud-based evaluation framework**. You'll learn how to:\n",
    "\n",
    "1. **Prepare test data** - Create queries and collect agent responses\n",
    "2. **Format data for evaluation** - Convert responses to evaluation-ready format\n",
    "3. **Configure evaluators** - Set up quality and agent-specific metrics\n",
    "4. **Run cloud evaluation** - Submit to Azure AI Foundry for comprehensive analysis\n",
    "5. **Analyze results** - Review metrics in the Azure AI Foundry portal\n",
    "\n",
    "### What You'll Evaluate\n",
    "\n",
    "The **Cooking AI Agent** is a conversational agent that helps users with:\n",
    "- Recipe search\n",
    "- Ingredient extraction\n",
    "- Recipe suggestions based on preferences\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "We'll measure both **general quality** and **agent-specific** performance:\n",
    "\n",
    "**Quality Metrics:**\n",
    "- **Relevance** (1-5): How well responses address the query\n",
    "- **Coherence** (1-5): Logical structure and clarity\n",
    "- **Fluency** (1-5): Grammatical correctness and readability\n",
    "\n",
    "**Agent Metrics:**\n",
    "- **Intent Resolution** (1-5): Understanding user intent\n",
    "- **Tool Call Accuracy** (1-5): Correct tool selection and parameters\n",
    "- **Task Adherence** (1-5): Following instructions and scope\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, ensure you have:\n",
    "- ✅ Azure AI Foundry project set up\n",
    "- ✅ Azure OpenAI resource deployed\n",
    "- ✅ Agent responses collected (from `run_agent.py`)\n",
    "- ✅ Environment variables configured\n",
    "\n",
    "Let's get started! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e649e6",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "\n",
    "First, let's import all the libraries needed for cloud-based evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb5e7a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import (\n",
    "    Evaluation,\n",
    "    InputDataset,\n",
    "    EvaluatorConfiguration,\n",
    "    EvaluatorIds\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(\"..\\..\\.env\")\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa007960",
   "metadata": {},
   "source": [
    "## Step 2: Define Tool Definitions\n",
    "\n",
    "For the **Tool Call Accuracy** evaluator to work, we need to provide the tool definitions that were available to the cooking agent. This helps the evaluator understand which tools exist and their intended use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2650ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Defined 3 tools for the cooking agent:\n",
      "   - search_recipes: Search for recipes based on a query. Returns matching recipe...\n",
      "   - extract_ingredients: Extract and return the full list of ingredients for a specif...\n",
      "   - get_recipe_suggestions: Get recipe suggestions based on dietary preferences or meal ...\n"
     ]
    }
   ],
   "source": [
    "def get_tool_definitions() -> list:\n",
    "    \"\"\"\n",
    "    Get the tool definitions that were available to the cooking agent.\n",
    "    These definitions help the Tool Call Accuracy evaluator understand\n",
    "    which tools were available and their intended use.\n",
    "    \n",
    "    Format matches Azure AI Foundry evaluator requirements.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        {\n",
    "            \"id\": \"search_recipes\",\n",
    "            \"name\": \"search_recipes\",\n",
    "            \"description\": \"Search for recipes based on a query. Returns matching recipes with their basic information.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The search query for recipes (e.g., 'pasta', 'chicken', 'dessert')\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"query\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"extract_ingredients\",\n",
    "            \"name\": \"extract_ingredients\",\n",
    "            \"description\": \"Extract and return the full list of ingredients for a specific recipe.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"recipe_name\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The name of the recipe to extract ingredients from\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"recipe_name\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"get_recipe_suggestions\",\n",
    "            \"name\": \"get_recipe_suggestions\",\n",
    "            \"description\": \"Get recipe suggestions based on dietary preferences or meal type.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"dietary_preference\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Dietary preference (e.g., 'quick', 'vegetarian', 'meat', 'dessert')\",\n",
    "                        \"default\": \"any\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "# Display tool definitions\n",
    "tools = get_tool_definitions()\n",
    "print(f\"✅ Defined {len(tools)} tools for the cooking agent:\")\n",
    "for tool in tools:\n",
    "    print(f\"   - {tool['name']}: {tool['description'][:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2196cf1",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Evaluation Data\n",
    "\n",
    "Now we'll convert the test responses (collected by running the agent) into JSONL format required by Azure AI Foundry's cloud evaluation.\n",
    "\n",
    "Each record will include:\n",
    "- `query`: The user's question\n",
    "- `response`: The agent's final answer\n",
    "- `tool_calls`: Tools used by the agent\n",
    "- `tool_definitions`: Available tools (for evaluation context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a303a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Preparing evaluation data from test_responses.json...\n",
      "✅ Created evaluation dataset: evaluation_data.jsonl (10 records)\n",
      "\n",
      "📋 Preview of first evaluation record:\n",
      "   Query: Find me some pasta recipes\n",
      "   Response: I found a delicious pasta recipe for you: Pasta Carbonara! It takes just 10 minutes to prep and 15 m...\n",
      "   Tool calls: 1 calls\n"
     ]
    }
   ],
   "source": [
    "def prepare_evaluation_data(responses_file: str, output_jsonl: str) -> None:\n",
    "    \"\"\"\n",
    "    Convert test responses to JSONL format required by cloud evaluation.\n",
    "    \n",
    "    Args:\n",
    "        responses_file: Path to test_responses.json\n",
    "        output_jsonl: Path to output JSONL file\n",
    "    \"\"\"\n",
    "    print(f\"📊 Preparing evaluation data from {responses_file}...\")\n",
    "    \n",
    "    with open(responses_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        responses = data.get(\"responses\", [])\n",
    "    \n",
    "    # Get tool definitions that were available to the agent\n",
    "    tool_definitions = get_tool_definitions()\n",
    "    \n",
    "    # Convert to JSONL format with required fields\n",
    "    with open(output_jsonl, 'w', encoding='utf-8') as f:\n",
    "        for item in responses:\n",
    "            # Create evaluation record\n",
    "            eval_record = {\n",
    "                \"query\": item.get(\"query\", \"\"),\n",
    "                \"response\": item.get(\"response\", \"\"),\n",
    "                \"tool_calls\": item.get(\"tool_calls\", []),  # Include tool calls for agent evaluators\n",
    "                \"tool_definitions\": tool_definitions  # Include tool definitions for Tool Call Accuracy evaluator\n",
    "            }\n",
    "            \n",
    "            f.write(json.dumps(eval_record) + '\\n')\n",
    "    \n",
    "    print(f\"✅ Created evaluation dataset: {output_jsonl} ({len(responses)} records)\")\n",
    "    return len(responses)\n",
    "\n",
    "# Prepare the data\n",
    "responses_file = \"test_responses.json\"\n",
    "eval_data_file = \"evaluation_data.jsonl\"\n",
    "\n",
    "num_records = prepare_evaluation_data(responses_file, eval_data_file)\n",
    "\n",
    "# Preview first record\n",
    "print(f\"\\n📋 Preview of first evaluation record:\")\n",
    "with open(eval_data_file, 'r', encoding='utf-8') as f:\n",
    "    first_record = json.loads(f.readline())\n",
    "    print(f\"   Query: {first_record['query']}\")\n",
    "    print(f\"   Response: {first_record['response'][:100]}...\")\n",
    "    print(f\"   Tool calls: {len(first_record['tool_calls'])} calls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d0f5cc",
   "metadata": {},
   "source": [
    "## Step 4: Configure Azure AI Foundry Project\n",
    "\n",
    "Let's connect to your Azure AI Foundry project where the evaluation will run. You'll need:\n",
    "- **PROJECT_ENDPOINT**: Your Azure AI Foundry project endpoint\n",
    "- **MODEL_ENDPOINT**: Your Azure OpenAI endpoint (for evaluator model)\n",
    "- **AZURE_OPENAI_DEPLOYMENT**: Model deployment name (default: gpt-4o-mini)\n",
    "\n",
    "**Authentication Options:**\n",
    "- **Option 1 (Recommended)**: Use Azure credentials (no API key needed) via `az login`\n",
    "- **Option 2**: Provide `AZURE_OPENAI_API_KEY` environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00796b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Configuring Azure AI Foundry project...\n",
      "✅ Project endpoint: https://foundrytkkzfewrtdpby.services.ai.azure.com/api/projects/foundry-tkkzfewrtdpby-project\n",
      "✅ Model endpoint: https://admin-mebeltoz-eastus2.openai.azure.com/\n",
      "✅ Model deployment: gpt-4.1\n"
     ]
    }
   ],
   "source": [
    "print(\"🔧 Configuring Azure AI Foundry project...\")\n",
    "\n",
    "# Get configuration from environment\n",
    "project_endpoint = os.getenv(\"AI_FOUNDRY_PROJECT_ENDPOINT\")\n",
    "model_endpoint = os.getenv(\"AOAI_ENDPOINT\")\n",
    "model_deployment_name = os.getenv(\"MODEL_DEPLOYMENT_NAME\", \"gpt-4o-mini\")\n",
    "\n",
    "# Validate configuration\n",
    "if not project_endpoint:\n",
    "    print(\"❌ PROJECT_ENDPOINT not found. Please set it in environment variables.\")\n",
    "    print(\"   Format: https://<account>.services.ai.azure.com/api/projects/<project>\")\n",
    "else:\n",
    "    print(f\"✅ Project endpoint: {project_endpoint}\")\n",
    "\n",
    "if not model_endpoint:\n",
    "    print(\"❌ MODEL_ENDPOINT not found. Please set it in environment variables.\")\n",
    "    print(\"   Format: https://<account>.services.ai.azure.com\")\n",
    "else:\n",
    "    print(f\"✅ Model endpoint: {model_endpoint}\")\n",
    "\n",
    "\n",
    "print(f\"✅ Model deployment: {model_deployment_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e10dcc",
   "metadata": {},
   "source": [
    "## Step 5: Create AI Project Client\n",
    "\n",
    "Connect to Azure AI Foundry using your credentials. Make sure you've run `az login` before this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11132904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌐 Connecting to Azure AI Foundry project...\n",
      "✅ Connected to Azure AI Foundry project\n"
     ]
    }
   ],
   "source": [
    "print(\"🌐 Connecting to Azure AI Foundry project...\")\n",
    "\n",
    "try:\n",
    "    project_client = AIProjectClient(\n",
    "        endpoint=project_endpoint,\n",
    "        credential=DefaultAzureCredential(),\n",
    "    )\n",
    "    print(\"✅ Connected to Azure AI Foundry project\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to connect to project: {e}\")\n",
    "    print(\"Please run 'az login' and ensure you have access to the project.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7cbd7f",
   "metadata": {},
   "source": [
    "## Step 6: Upload Evaluation Dataset\n",
    "\n",
    "Upload the JSONL file to Azure AI Foundry as a dataset. This creates a versioned dataset that can be reused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "439694a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📤 Uploading evaluation dataset...\n",
      "✅ Dataset uploaded: cooking-agent-test-data (v20251029-223733)\n",
      "   Dataset ID: azureai://accounts/foundry-tkkzfewrtdpby/projects/foundry-tkkzfewrtdpby-project/data/cooking-agent-test-data/versions/20251029-223733\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"cooking-agent-test-data\"\n",
    "# Use timestamp for unique version to avoid conflicts\n",
    "dataset_version = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "print(f\"📤 Uploading evaluation dataset...\")\n",
    "\n",
    "try:\n",
    "    data_upload = project_client.datasets.upload_file(\n",
    "        name=dataset_name,\n",
    "        version=dataset_version,\n",
    "        file_path=eval_data_file,\n",
    "    )\n",
    "    data_id = data_upload.id\n",
    "    if not data_id:\n",
    "        print(\"❌ Dataset upload succeeded but no ID returned\")\n",
    "        raise Exception(\"No dataset ID returned\")\n",
    "    \n",
    "    print(f\"✅ Dataset uploaded: {dataset_name} (v{dataset_version})\")\n",
    "    print(f\"   Dataset ID: {data_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to upload dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e7d03a",
   "metadata": {},
   "source": [
    "## Step 7: Configure Evaluators\n",
    "\n",
    "Now we'll configure both **quality evaluators** and **agent-specific evaluators**:\n",
    "\n",
    "### Quality Evaluators (General)\n",
    "- **Relevance**: Does the response address the query?\n",
    "- **Coherence**: Is the response logically structured?\n",
    "- **Fluency**: Is the response grammatically correct?\n",
    "\n",
    "### Agent Evaluators (Agent-Specific)\n",
    "- **Intent Resolution**: Does the agent understand user intent?\n",
    "- **Tool Call Accuracy**: Are tools used correctly?\n",
    "- **Task Adherence**: Does the agent stay within task scope?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca5d66b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Configuring evaluators...\n",
      "✅ Configured 6 evaluators:\n",
      "   Quality Evaluators:\n",
      "   - relevance\n",
      "   - coherence\n",
      "   - fluency\n",
      "   Agent Evaluators:\n",
      "   - intent_resolution\n",
      "   - tool_call_accuracy\n",
      "   - task_adherence\n"
     ]
    }
   ],
   "source": [
    "print(\"📋 Configuring evaluators...\")\n",
    "\n",
    "evaluators = {\n",
    "    # Quality evaluators\n",
    "    \"relevance\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.RELEVANCE.value,\n",
    "        init_params={\"deployment_name\": model_deployment_name},\n",
    "        data_mapping={\n",
    "            \"query\": \"${data.query}\",\n",
    "            \"response\": \"${data.response}\",\n",
    "        },\n",
    "    ),\n",
    "    \"coherence\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.COHERENCE.value,\n",
    "        init_params={\"deployment_name\": model_deployment_name},\n",
    "        data_mapping={\n",
    "            \"query\": \"${data.query}\",\n",
    "            \"response\": \"${data.response}\",\n",
    "        },\n",
    "    ),\n",
    "    \"fluency\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.FLUENCY.value,\n",
    "        init_params={\"deployment_name\": model_deployment_name},\n",
    "        data_mapping={\n",
    "            \"query\": \"${data.query}\",\n",
    "            \"response\": \"${data.response}\",\n",
    "        },\n",
    "    ),\n",
    "    # Agent-specific evaluators\n",
    "    \"intent_resolution\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.INTENT_RESOLUTION.value,\n",
    "        init_params={\"deployment_name\": model_deployment_name},\n",
    "        data_mapping={\n",
    "            \"query\": \"${data.query}\",\n",
    "            \"response\": \"${data.response}\",\n",
    "        },\n",
    "    ),\n",
    "    \"tool_call_accuracy\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.TOOL_CALL_ACCURACY.value,\n",
    "        init_params={\"deployment_name\": model_deployment_name},\n",
    "        data_mapping={\n",
    "            \"query\": \"${data.query}\",\n",
    "            \"response\": \"${data.response}\",\n",
    "            \"tool_calls\": \"${data.tool_calls}\",  # Map tool_calls from data\n",
    "            \"tool_definitions\": \"${data.tool_definitions}\",  # Map tool definitions from data\n",
    "        },\n",
    "    ),\n",
    "    \"task_adherence\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.TASK_ADHERENCE.value,\n",
    "        init_params={\"deployment_name\": model_deployment_name},\n",
    "        data_mapping={\n",
    "            \"query\": \"${data.query}\",\n",
    "            \"response\": \"${data.response}\",\n",
    "        },\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(f\"✅ Configured {len(evaluators)} evaluators:\")\n",
    "print(\"   Quality Evaluators:\")\n",
    "print(\"   - relevance\")\n",
    "print(\"   - coherence\")\n",
    "print(\"   - fluency\")\n",
    "print(\"   Agent Evaluators:\")\n",
    "print(\"   - intent_resolution\")\n",
    "print(\"   - tool_call_accuracy\")\n",
    "print(\"   - task_adherence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5fc6f1",
   "metadata": {},
   "source": [
    "## Step 8: Create and Submit Evaluation\n",
    "\n",
    "Now we'll create the evaluation job and submit it to Azure AI Foundry. The evaluation will run in the cloud using the configured evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "826e5418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Submitting cloud evaluation...\n",
      "   Using Azure credential authentication\n",
      "✅ Evaluation submitted successfully!\n",
      "======================================================================\n",
      "📊 Evaluation Details:\n",
      "   Name: 8c01712b-5e44-4f87-82a4-d524c86d2ce9\n",
      "   Status: NotStarted\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"🚀 Submitting cloud evaluation...\")\n",
    "\n",
    "\n",
    "try:\n",
    "    evaluation = Evaluation(\n",
    "        display_name=\"Cooking Agent Evaluation\",\n",
    "        description=\"Evaluation of cooking agent responses for quality (relevance, coherence, fluency) and agent-specific metrics (intent resolution, tool call accuracy, task adherence)\",\n",
    "        data=InputDataset(id=data_id),\n",
    "        evaluators=evaluators,\n",
    "    )\n",
    "    \n",
    "    # Prepare headers based on authentication method\n",
    "    headers = {\"model-endpoint\": model_endpoint}\n",
    "    \n",
    "    #if model_api_key:\n",
    "        # Use API key authentication\n",
    "    #    headers[\"api-key\"] = model_api_key\n",
    "    #    print(\"   Using API key authentication\")\n",
    "    #else:\n",
    "    # Use Azure credential authentication (requires proper RBAC)\n",
    "    from azure.identity import get_bearer_token_provider\n",
    "    credential = DefaultAzureCredential()\n",
    "    token_provider = get_bearer_token_provider(\n",
    "        credential,\n",
    "        \"https://cognitiveservices.azure.com/.default\"\n",
    "    )\n",
    "    # Get token and add to headers\n",
    "    token = token_provider()\n",
    "    headers[\"Authorization\"] = f\"Bearer {token}\"\n",
    "    print(\"   Using Azure credential authentication\")\n",
    "\n",
    "    # Submit the evaluation\n",
    "    evaluation_response = project_client.evaluations.create(\n",
    "        evaluation,\n",
    "        headers=headers,\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Evaluation submitted successfully!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"📊 Evaluation Details:\")\n",
    "    if hasattr(evaluation_response, 'name'):\n",
    "        print(f\"   Name: {evaluation_response.name}\")\n",
    "    if hasattr(evaluation_response, 'status'):\n",
    "        print(f\"   Status: {evaluation_response.status}\")\n",
    "    if hasattr(evaluation_response, 'id'):\n",
    "        print(f\"   ID: {evaluation_response.id}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Failed to submit evaluation: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Ensure PROJECT_ENDPOINT is correct\")\n",
    "    print(\"2. Ensure MODEL_ENDPOINT is correct\")\n",
    "    print(\"3. Verify model deployment exists in your Azure OpenAI resource\")\n",
    "    print(\"4. Check that storage account is connected to your project\")\n",
    "    print(\"5. Ensure you have appropriate RBAC permissions:\")\n",
    "    print(\"   - Cognitive Services OpenAI User (for Azure credential auth)\")\n",
    "    print(\"   - Or use AZURE_OPENAI_API_KEY environment variable\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaf5c3d",
   "metadata": {},
   "source": [
    "## Step 9: View Results in Azure AI Foundry Portal\n",
    "\n",
    "Your evaluation is now running in the cloud! 🎉\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Open Azure AI Foundry Portal**\n",
    "   - Navigate to: https://ai.azure.com\n",
    "   \n",
    "2. **Find Your Project**\n",
    "   - Go to your project dashboard\n",
    "   - Click on the **Evaluation** tab\n",
    "   \n",
    "3. **View Results**\n",
    "   - Look for evaluation: **\"Cooking Agent Evaluation\"**\n",
    "   - Dataset version: **{dataset_version}**\n",
    "   - View metrics, charts, and detailed results\n",
    "   \n",
    "4. **Analyze Metrics**\n",
    "   - Quality scores (1-5): Relevance, Coherence, Fluency\n",
    "   - Agent scores (1-5): Intent Resolution, Tool Call Accuracy, Task Adherence\n",
    "   - Per-query breakdowns and aggregated statistics\n",
    "\n",
    "### Understanding Results\n",
    "\n",
    "**Score Scale**: All evaluators use a 1-5 scale where:\n",
    "- **5** = Excellent\n",
    "- **4** = Good\n",
    "- **3** = Acceptable\n",
    "- **2** = Needs improvement\n",
    "- **1** = Poor\n",
    "\n",
    "**What to Look For**:\n",
    "- High scores (4-5) across all metrics indicate strong performance\n",
    "- Low tool call accuracy may indicate incorrect tool selection\n",
    "- Low intent resolution suggests the agent misunderstands queries\n",
    "- Low task adherence means the agent goes off-topic\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Congratulations! 🎊 You've completed the cloud evaluation lab and learned how to:\n",
    "\n",
    "✅ Define tool definitions for agent evaluation  \n",
    "✅ Prepare evaluation data in JSONL format  \n",
    "✅ Configure Azure AI Foundry project  \n",
    "✅ Upload datasets to the cloud  \n",
    "✅ Set up quality and agent-specific evaluators  \n",
    "✅ Submit cloud evaluations  \n",
    "✅ View results in the portal  \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Cloud evaluation** scales better than local evaluation\n",
    "2. **Agent evaluators** provide insights specific to tool-calling agents\n",
    "3. **Versioned datasets** enable tracking improvements over time\n",
    "4. **Azure AI Foundry** provides rich visualizations and historical tracking\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Iterate on agent prompts** based on evaluation results\n",
    "- **Add more test queries** to cover edge cases\n",
    "- **Compare evaluations** across different versions\n",
    "- **Set up automated evaluation** in your CI/CD pipeline\n",
    "\n",
    "Happy evaluating! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5d1b78",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🔬 Optional: Explore the Data\n",
    "\n",
    "Want to understand what data looks like at each step? Run the cells below to inspect the evaluation pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a937132",
   "metadata": {},
   "source": [
    "### View Test Queries\n",
    "\n",
    "These are the queries used to test the cooking agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce2aa881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Test Queries (10 total):\n",
      "\n",
      "1. Find me some pasta recipes\n",
      "2. What ingredients do I need for carbonara?\n",
      "3. I want to make something with chicken\n",
      "4. Suggest some quick recipes for dinner\n",
      "5. Do you have any soup recipes?\n",
      "6. What can I make for dessert?\n",
      "7. Show me vegetarian options\n",
      "8. I need ingredients for chicken stir fry\n",
      "9. What recipes use tomatoes?\n",
      "10. Give me some meat-based recipes\n"
     ]
    }
   ],
   "source": [
    "# Load and display test queries\n",
    "with open('test_queries.json', 'r', encoding='utf-8') as f:\n",
    "    queries_data = json.load(f)\n",
    "    queries = queries_data.get(\"queries\", [])\n",
    "\n",
    "print(f\"📝 Test Queries ({len(queries)} total):\\n\")\n",
    "for i, q in enumerate(queries, 1):\n",
    "    print(f\"{i}. {q['query']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b4f9b1",
   "metadata": {},
   "source": [
    "### View Agent Responses\n",
    "\n",
    "Let's look at a sample response from the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b322636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📤 Sample Response:\n",
      "\n",
      "Query: Find me some pasta recipes\n",
      "\n",
      "Response: I found a delicious pasta recipe for you: Pasta Carbonara! It takes just 10 minutes to prep and 15 minutes to cook, making it a quick and tasty option. Would you like the full ingredient list or step-by-step cooking instructions for Pasta Carbonara? If you’re interested in more pasta recipes or have specific preferences (like vegetarian or gluten-free), let me know!\n",
      "\n",
      "Tool Calls: 1 calls\n",
      "\n",
      "Tool Call Details:\n",
      "  1. search_recipes({'query': 'pasta'})\n"
     ]
    }
   ],
   "source": [
    "# Load and display a sample response\n",
    "with open('test_responses.json', 'r', encoding='utf-8') as f:\n",
    "    responses_data = json.load(f)\n",
    "    responses = responses_data.get(\"responses\", [])\n",
    "\n",
    "# Show first response\n",
    "if responses:\n",
    "    sample = responses[0]\n",
    "    print(f\"📤 Sample Response:\\n\")\n",
    "    print(f\"Query: {sample['query']}\")\n",
    "    print(f\"\\nResponse: {sample['response']}\")\n",
    "    print(f\"\\nTool Calls: {len(sample.get('tool_calls', []))} calls\")\n",
    "    \n",
    "    if sample.get('tool_calls'):\n",
    "        print(\"\\nTool Call Details:\")\n",
    "        for i, tc in enumerate(sample['tool_calls'], 1):\n",
    "            print(f\"  {i}. {tc.get('name', 'unknown')}({tc.get('arguments', {})})\")\n",
    "else:\n",
    "    print(\"⚠️ No responses found. Run 'python run_agent.py' first to collect responses.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5816ae57",
   "metadata": {},
   "source": [
    "### View Evaluation Data Format\n",
    "\n",
    "Here's what the formatted evaluation data looks like (JSONL format):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e3cf174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Evaluation Data Format (JSONL):\n",
      "\n",
      "{\n",
      "  \"query\": \"Find me some pasta recipes\",\n",
      "  \"response\": \"I found a delicious pasta recipe for you: Pasta Carbonara! It takes just 10 minutes to prep and 15 m...\",\n",
      "  \"tool_calls\": [\n",
      "    {\n",
      "      \"type\": \"tool_call\",\n",
      "      \"tool_call_id\": \"call_uy2o6m70jy00xddGS0wkA7HG\",\n",
      "      \"name\": \"search_recipes\",\n",
      "      \"arguments\": {\n",
      "        \"query\": \"pasta\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"tool_definitions_count\": 3\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Read and display first evaluation record\n",
    "import json\n",
    "\n",
    "print(\"📄 Evaluation Data Format (JSONL):\\n\")\n",
    "with open('evaluation_data.jsonl', 'r', encoding='utf-8') as f:\n",
    "    first_line = f.readline()\n",
    "    eval_record = json.loads(first_line)\n",
    "    \n",
    "    # Pretty print the structure\n",
    "    print(json.dumps({\n",
    "        \"query\": eval_record[\"query\"],\n",
    "        \"response\": eval_record[\"response\"][:100] + \"...\" if len(eval_record[\"response\"]) > 100 else eval_record[\"response\"],\n",
    "        \"tool_calls\": eval_record[\"tool_calls\"],\n",
    "        \"tool_definitions_count\": len(eval_record[\"tool_definitions\"])\n",
    "    }, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
