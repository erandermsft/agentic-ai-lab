{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b04245fc",
   "metadata": {},
   "source": [
    "# üî¨ Lab: Evaluating the Cooking AI Agent\n",
    "\n",
    "## Overview\n",
    "\n",
    "This lab walks you through the complete process of evaluating an AI agent using **Azure AI Foundry's cloud-based evaluation framework**. You'll learn how to:\n",
    "\n",
    "1. **Prepare test data** - Create queries and collect agent responses\n",
    "2. **Format data for evaluation** - Convert responses to evaluation-ready format\n",
    "3. **Configure evaluators** - Set up quality and agent-specific metrics\n",
    "4. **Run cloud evaluation** - Submit to Azure AI Foundry for comprehensive analysis\n",
    "5. **Analyze results** - Review metrics in the Azure AI Foundry portal\n",
    "\n",
    "### What You'll Evaluate\n",
    "\n",
    "The **Cooking AI Agent** is a conversational agent that helps users with:\n",
    "- Recipe search\n",
    "- Ingredient extraction\n",
    "- Recipe suggestions based on preferences\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "We'll measure both **general quality** and **agent-specific** performance:\n",
    "\n",
    "**Quality Metrics:**\n",
    "- **Relevance** (1-5): How well responses address the query\n",
    "- **Coherence** (1-5): Logical structure and clarity\n",
    "- **Fluency** (1-5): Grammatical correctness and readability\n",
    "\n",
    "**Agent Metrics:**\n",
    "- **Intent Resolution** (1-5): Understanding user intent\n",
    "- **Tool Call Accuracy** (1-5): Correct tool selection and parameters\n",
    "- **Task Adherence** (1-5): Following instructions and scope\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, ensure you have:\n",
    "- ‚úÖ Azure AI Foundry project set up\n",
    "- ‚úÖ Azure OpenAI resource deployed\n",
    "- ‚úÖ Agent responses collected (from `run_agent.py`)\n",
    "- ‚úÖ Environment variables configured\n",
    "\n",
    "Let's get started! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e649e6",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "\n",
    "First, let's import all the libraries needed for cloud-based evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb5e7a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import (\n",
    "    Evaluation,\n",
    "    InputDataset,\n",
    "    EvaluatorConfiguration,\n",
    "    EvaluatorIds\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(\"..\\..\\.env\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa007960",
   "metadata": {},
   "source": [
    "## Step 2: Define Tool Definitions\n",
    "\n",
    "For the **Tool Call Accuracy** evaluator to work, we need to provide the tool definitions that were available to the cooking agent. This helps the evaluator understand which tools exist and their intended use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2650ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Defined 3 tools for the cooking agent:\n",
      "   - search_recipes: Search for recipes based on a query. Returns matching recipe...\n",
      "   - extract_ingredients: Extract and return the full list of ingredients for a specif...\n",
      "   - get_recipe_suggestions: Get recipe suggestions based on dietary preferences or meal ...\n"
     ]
    }
   ],
   "source": [
    "def get_tool_definitions() -> list:\n",
    "    \"\"\"\n",
    "    Get the tool definitions that were available to the cooking agent.\n",
    "    These definitions help the Tool Call Accuracy evaluator understand\n",
    "    which tools were available and their intended use.\n",
    "    \n",
    "    Format matches Azure AI Foundry evaluator requirements.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        {\n",
    "            \"id\": \"search_recipes\",\n",
    "            \"name\": \"search_recipes\",\n",
    "            \"description\": \"Search for recipes based on a query. Returns matching recipes with their basic information.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The search query for recipes (e.g., 'pasta', 'chicken', 'dessert')\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"query\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"extract_ingredients\",\n",
    "            \"name\": \"extract_ingredients\",\n",
    "            \"description\": \"Extract and return the full list of ingredients for a specific recipe.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"recipe_name\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The name of the recipe to extract ingredients from\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"recipe_name\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"get_recipe_suggestions\",\n",
    "            \"name\": \"get_recipe_suggestions\",\n",
    "            \"description\": \"Get recipe suggestions based on dietary preferences or meal type.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"dietary_preference\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Dietary preference (e.g., 'quick', 'vegetarian', 'meat', 'dessert')\",\n",
    "                        \"default\": \"any\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "# Display tool definitions\n",
    "tools = get_tool_definitions()\n",
    "print(f\"‚úÖ Defined {len(tools)} tools for the cooking agent:\")\n",
    "for tool in tools:\n",
    "    print(f\"   - {tool['name']}: {tool['description'][:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2196cf1",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Evaluation Data\n",
    "\n",
    "Now we'll convert the test responses (collected by running the agent) into JSONL format required by Azure AI Foundry's cloud evaluation.\n",
    "\n",
    "Each record will include:\n",
    "- `query`: The user's question\n",
    "- `response`: The agent's final answer\n",
    "- `tool_calls`: Tools used by the agent\n",
    "- `tool_definitions`: Available tools (for evaluation context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a303a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Preparing evaluation data from test_responses.json...\n",
      "‚úÖ Created evaluation dataset: evaluation_data.jsonl (10 records)\n",
      "\n",
      "üìã Preview of first evaluation record:\n",
      "   Query: Find me some pasta recipes\n",
      "   Response: I found a delicious pasta recipe for you: Pasta Carbonara! It takes just 10 minutes to prep and 15 m...\n",
      "   Tool calls: 1 calls\n"
     ]
    }
   ],
   "source": [
    "def prepare_evaluation_data(responses_file: str, output_jsonl: str) -> None:\n",
    "    \"\"\"\n",
    "    Convert test responses to JSONL format required by cloud evaluation.\n",
    "    \n",
    "    Args:\n",
    "        responses_file: Path to test_responses.json\n",
    "        output_jsonl: Path to output JSONL file\n",
    "    \"\"\"\n",
    "    print(f\"üìä Preparing evaluation data from {responses_file}...\")\n",
    "    \n",
    "    with open(responses_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        responses = data.get(\"responses\", [])\n",
    "    \n",
    "    # Get tool definitions that were available to the agent\n",
    "    tool_definitions = get_tool_definitions()\n",
    "    \n",
    "    # Convert to JSONL format with required fields\n",
    "    with open(output_jsonl, 'w', encoding='utf-8') as f:\n",
    "        for item in responses:\n",
    "            # Create evaluation record\n",
    "            eval_record = {\n",
    "                \"query\": item.get(\"query\", \"\"),\n",
    "                \"response\": item.get(\"response\", \"\"),\n",
    "                \"tool_calls\": item.get(\"tool_calls\", []),  # Include tool calls for agent evaluators\n",
    "                \"tool_definitions\": tool_definitions  # Include tool definitions for Tool Call Accuracy evaluator\n",
    "            }\n",
    "            \n",
    "            f.write(json.dumps(eval_record) + '\\n')\n",
    "    \n",
    "    print(f\"‚úÖ Created evaluation dataset: {output_jsonl} ({len(responses)} records)\")\n",
    "    return len(responses)\n",
    "\n",
    "# Prepare the data\n",
    "responses_file = \"test_responses.json\"\n",
    "eval_data_file = \"evaluation_data.jsonl\"\n",
    "\n",
    "num_records = prepare_evaluation_data(responses_file, eval_data_file)\n",
    "\n",
    "# Preview first record\n",
    "print(f\"\\nüìã Preview of first evaluation record:\")\n",
    "with open(eval_data_file, 'r', encoding='utf-8') as f:\n",
    "    first_record = json.loads(f.readline())\n",
    "    print(f\"   Query: {first_record['query']}\")\n",
    "    print(f\"   Response: {first_record['response'][:100]}...\")\n",
    "    print(f\"   Tool calls: {len(first_record['tool_calls'])} calls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d0f5cc",
   "metadata": {},
   "source": [
    "## Step 4: Configure Azure AI Foundry Project\n",
    "\n",
    "Let's connect to your Azure AI Foundry project where the evaluation will run. You'll need:\n",
    "- **PROJECT_ENDPOINT**: Your Azure AI Foundry project endpoint\n",
    "- **MODEL_ENDPOINT**: Your Azure OpenAI endpoint (for evaluator model)\n",
    "- **AZURE_OPENAI_DEPLOYMENT**: Model deployment name (default: gpt-4o-mini)\n",
    "\n",
    "**Authentication Options:**\n",
    "- **Option 1 (Recommended)**: Use Azure credentials (no API key needed) via `az login`\n",
    "- **Option 2**: Provide `AZURE_OPENAI_API_KEY` environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00796b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Configuring Azure AI Foundry project...\n",
      "‚úÖ Project endpoint: https://foundrytkkzfewrtdpby.services.ai.azure.com/api/projects/foundry-tkkzfewrtdpby-project\n",
      "‚úÖ Model endpoint: https://admin-mebeltoz-eastus2.openai.azure.com/\n",
      "‚úÖ Model deployment: gpt-4.1\n"
     ]
    }
   ],
   "source": [
    "print(\"üîß Configuring Azure AI Foundry project...\")\n",
    "\n",
    "# Get configuration from environment\n",
    "project_endpoint = os.getenv(\"AI_FOUNDRY_PROJECT_ENDPOINT\")\n",
    "model_endpoint = os.getenv(\"AOAI_ENDPOINT\")\n",
    "model_deployment_name = os.getenv(\"MODEL_DEPLOYMENT_NAME\", \"gpt-4o-mini\")\n",
    "\n",
    "# Validate configuration\n",
    "if not project_endpoint:\n",
    "    print(\"‚ùå PROJECT_ENDPOINT not found. Please set it in environment variables.\")\n",
    "    print(\"   Format: https://<account>.services.ai.azure.com/api/projects/<project>\")\n",
    "else:\n",
    "    print(f\"‚úÖ Project endpoint: {project_endpoint}\")\n",
    "\n",
    "if not model_endpoint:\n",
    "    print(\"‚ùå MODEL_ENDPOINT not found. Please set it in environment variables.\")\n",
    "    print(\"   Format: https://<account>.services.ai.azure.com\")\n",
    "else:\n",
    "    print(f\"‚úÖ Model endpoint: {model_endpoint}\")\n",
    "\n",
    "\n",
    "print(f\"‚úÖ Model deployment: {model_deployment_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e10dcc",
   "metadata": {},
   "source": [
    "## Step 5: Create AI Project Client\n",
    "\n",
    "Connect to Azure AI Foundry using your credentials. Make sure you've run `az login` before this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11132904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Connecting to Azure AI Foundry project...\n",
      "‚úÖ Connected to Azure AI Foundry project\n"
     ]
    }
   ],
   "source": [
    "print(\"üåê Connecting to Azure AI Foundry project...\")\n",
    "\n",
    "try:\n",
    "    project_client = AIProjectClient(\n",
    "        endpoint=project_endpoint,\n",
    "        credential=DefaultAzureCredential(),\n",
    "    )\n",
    "    print(\"‚úÖ Connected to Azure AI Foundry project\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to connect to project: {e}\")\n",
    "    print(\"Please run 'az login' and ensure you have access to the project.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7cbd7f",
   "metadata": {},
   "source": [
    "## Step 6: Upload Evaluation Dataset\n",
    "\n",
    "Upload the JSONL file to Azure AI Foundry as a dataset. This creates a versioned dataset that can be reused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "439694a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Uploading evaluation dataset...\n",
      "‚úÖ Dataset uploaded: cooking-agent-test-data (v20251029-223733)\n",
      "   Dataset ID: azureai://accounts/foundry-tkkzfewrtdpby/projects/foundry-tkkzfewrtdpby-project/data/cooking-agent-test-data/versions/20251029-223733\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"cooking-agent-test-data\"\n",
    "# Use timestamp for unique version to avoid conflicts\n",
    "dataset_version = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "print(f\"üì§ Uploading evaluation dataset...\")\n",
    "\n",
    "try:\n",
    "    data_upload = project_client.datasets.upload_file(\n",
    "        name=dataset_name,\n",
    "        version=dataset_version,\n",
    "        file_path=eval_data_file,\n",
    "    )\n",
    "    data_id = data_upload.id\n",
    "    if not data_id:\n",
    "        print(\"‚ùå Dataset upload succeeded but no ID returned\")\n",
    "        raise Exception(\"No dataset ID returned\")\n",
    "    \n",
    "    print(f\"‚úÖ Dataset uploaded: {dataset_name} (v{dataset_version})\")\n",
    "    print(f\"   Dataset ID: {data_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to upload dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e7d03a",
   "metadata": {},
   "source": [
    "## Step 7: Configure Evaluators\n",
    "\n",
    "Now we'll configure both **quality evaluators** and **agent-specific evaluators**:\n",
    "\n",
    "### Quality Evaluators (General)\n",
    "- **Relevance**: Does the response address the query?\n",
    "- **Coherence**: Is the response logically structured?\n",
    "- **Fluency**: Is the response grammatically correct?\n",
    "\n",
    "### Agent Evaluators (Agent-Specific)\n",
    "- **Intent Resolution**: Does the agent understand user intent?\n",
    "- **Tool Call Accuracy**: Are tools used correctly?\n",
    "- **Task Adherence**: Does the agent stay within task scope?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca5d66b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Configuring evaluators...\n",
      "‚úÖ Configured 6 evaluators:\n",
      "   Quality Evaluators:\n",
      "   - relevance\n",
      "   - coherence\n",
      "   - fluency\n",
      "   Agent Evaluators:\n",
      "   - intent_resolution\n",
      "   - tool_call_accuracy\n",
      "   - task_adherence\n"
     ]
    }
   ],
   "source": [
    "print(\"üìã Configuring evaluators...\")\n",
    "\n",
    "evaluators = {\n",
    "    # Quality evaluators\n",
    "    \"relevance\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.RELEVANCE.value,\n",
    "        init_params={\"deployment_name\": model_deployment_name},\n",
    "        data_mapping={\n",
    "            \"query\": \"${data.query}\",\n",
    "            \"response\": \"${data.response}\",\n",
    "        },\n",
    "    ),\n",
    "    \"coherence\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.COHERENCE.value,\n",
    "        init_params={\"deployment_name\": model_deployment_name},\n",
    "        data_mapping={\n",
    "            \"query\": \"${data.query}\",\n",
    "            \"response\": \"${data.response}\",\n",
    "        },\n",
    "    ),\n",
    "    \"fluency\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.FLUENCY.value,\n",
    "        init_params={\"deployment_name\": model_deployment_name},\n",
    "        data_mapping={\n",
    "            \"query\": \"${data.query}\",\n",
    "            \"response\": \"${data.response}\",\n",
    "        },\n",
    "    ),\n",
    "    # Agent-specific evaluators\n",
    "    \"intent_resolution\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.INTENT_RESOLUTION.value,\n",
    "        init_params={\"deployment_name\": model_deployment_name},\n",
    "        data_mapping={\n",
    "            \"query\": \"${data.query}\",\n",
    "            \"response\": \"${data.response}\",\n",
    "        },\n",
    "    ),\n",
    "    \"tool_call_accuracy\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.TOOL_CALL_ACCURACY.value,\n",
    "        init_params={\"deployment_name\": model_deployment_name},\n",
    "        data_mapping={\n",
    "            \"query\": \"${data.query}\",\n",
    "            \"response\": \"${data.response}\",\n",
    "            \"tool_calls\": \"${data.tool_calls}\",  # Map tool_calls from data\n",
    "            \"tool_definitions\": \"${data.tool_definitions}\",  # Map tool definitions from data\n",
    "        },\n",
    "    ),\n",
    "    \"task_adherence\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.TASK_ADHERENCE.value,\n",
    "        init_params={\"deployment_name\": model_deployment_name},\n",
    "        data_mapping={\n",
    "            \"query\": \"${data.query}\",\n",
    "            \"response\": \"${data.response}\",\n",
    "        },\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Configured {len(evaluators)} evaluators:\")\n",
    "print(\"   Quality Evaluators:\")\n",
    "print(\"   - relevance\")\n",
    "print(\"   - coherence\")\n",
    "print(\"   - fluency\")\n",
    "print(\"   Agent Evaluators:\")\n",
    "print(\"   - intent_resolution\")\n",
    "print(\"   - tool_call_accuracy\")\n",
    "print(\"   - task_adherence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5fc6f1",
   "metadata": {},
   "source": [
    "## Step 8: Create and Submit Evaluation\n",
    "\n",
    "Now we'll create the evaluation job and submit it to Azure AI Foundry. The evaluation will run in the cloud using the configured evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "826e5418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Submitting cloud evaluation...\n",
      "   Using Azure credential authentication\n",
      "‚úÖ Evaluation submitted successfully!\n",
      "======================================================================\n",
      "üìä Evaluation Details:\n",
      "   Name: 8c01712b-5e44-4f87-82a4-d524c86d2ce9\n",
      "   Status: NotStarted\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ Submitting cloud evaluation...\")\n",
    "\n",
    "\n",
    "try:\n",
    "    evaluation = Evaluation(\n",
    "        display_name=\"Cooking Agent Evaluation\",\n",
    "        description=\"Evaluation of cooking agent responses for quality (relevance, coherence, fluency) and agent-specific metrics (intent resolution, tool call accuracy, task adherence)\",\n",
    "        data=InputDataset(id=data_id),\n",
    "        evaluators=evaluators,\n",
    "    )\n",
    "    \n",
    "    # Prepare headers based on authentication method\n",
    "    headers = {\"model-endpoint\": model_endpoint}\n",
    "    \n",
    "    #if model_api_key:\n",
    "        # Use API key authentication\n",
    "    #    headers[\"api-key\"] = model_api_key\n",
    "    #    print(\"   Using API key authentication\")\n",
    "    #else:\n",
    "    # Use Azure credential authentication (requires proper RBAC)\n",
    "    from azure.identity import get_bearer_token_provider\n",
    "    credential = DefaultAzureCredential()\n",
    "    token_provider = get_bearer_token_provider(\n",
    "        credential,\n",
    "        \"https://cognitiveservices.azure.com/.default\"\n",
    "    )\n",
    "    # Get token and add to headers\n",
    "    token = token_provider()\n",
    "    headers[\"Authorization\"] = f\"Bearer {token}\"\n",
    "    print(\"   Using Azure credential authentication\")\n",
    "\n",
    "    # Submit the evaluation\n",
    "    evaluation_response = project_client.evaluations.create(\n",
    "        evaluation,\n",
    "        headers=headers,\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Evaluation submitted successfully!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"üìä Evaluation Details:\")\n",
    "    if hasattr(evaluation_response, 'name'):\n",
    "        print(f\"   Name: {evaluation_response.name}\")\n",
    "    if hasattr(evaluation_response, 'status'):\n",
    "        print(f\"   Status: {evaluation_response.status}\")\n",
    "    if hasattr(evaluation_response, 'id'):\n",
    "        print(f\"   ID: {evaluation_response.id}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Failed to submit evaluation: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Ensure PROJECT_ENDPOINT is correct\")\n",
    "    print(\"2. Ensure MODEL_ENDPOINT is correct\")\n",
    "    print(\"3. Verify model deployment exists in your Azure OpenAI resource\")\n",
    "    print(\"4. Check that storage account is connected to your project\")\n",
    "    print(\"5. Ensure you have appropriate RBAC permissions:\")\n",
    "    print(\"   - Cognitive Services OpenAI User (for Azure credential auth)\")\n",
    "    print(\"   - Or use AZURE_OPENAI_API_KEY environment variable\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaf5c3d",
   "metadata": {},
   "source": [
    "## Step 9: View Results in Azure AI Foundry Portal\n",
    "\n",
    "Your evaluation is now running in the cloud! üéâ\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Open Azure AI Foundry Portal**\n",
    "   - Navigate to: https://ai.azure.com\n",
    "   \n",
    "2. **Find Your Project**\n",
    "   - Go to your project dashboard\n",
    "   - Click on the **Evaluation** tab\n",
    "   \n",
    "3. **View Results**\n",
    "   - Look for evaluation: **\"Cooking Agent Evaluation\"**\n",
    "   - Dataset version: **{dataset_version}**\n",
    "   - View metrics, charts, and detailed results\n",
    "   \n",
    "4. **Analyze Metrics**\n",
    "   - Quality scores (1-5): Relevance, Coherence, Fluency\n",
    "   - Agent scores (1-5): Intent Resolution, Tool Call Accuracy, Task Adherence\n",
    "   - Per-query breakdowns and aggregated statistics\n",
    "\n",
    "### Understanding Results\n",
    "\n",
    "**Score Scale**: All evaluators use a 1-5 scale where:\n",
    "- **5** = Excellent\n",
    "- **4** = Good\n",
    "- **3** = Acceptable\n",
    "- **2** = Needs improvement\n",
    "- **1** = Poor\n",
    "\n",
    "**What to Look For**:\n",
    "- High scores (4-5) across all metrics indicate strong performance\n",
    "- Low tool call accuracy may indicate incorrect tool selection\n",
    "- Low intent resolution suggests the agent misunderstands queries\n",
    "- Low task adherence means the agent goes off-topic\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Congratulations! üéä You've completed the cloud evaluation lab and learned how to:\n",
    "\n",
    "‚úÖ Define tool definitions for agent evaluation  \n",
    "‚úÖ Prepare evaluation data in JSONL format  \n",
    "‚úÖ Configure Azure AI Foundry project  \n",
    "‚úÖ Upload datasets to the cloud  \n",
    "‚úÖ Set up quality and agent-specific evaluators  \n",
    "‚úÖ Submit cloud evaluations  \n",
    "‚úÖ View results in the portal  \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Cloud evaluation** scales better than local evaluation\n",
    "2. **Agent evaluators** provide insights specific to tool-calling agents\n",
    "3. **Versioned datasets** enable tracking improvements over time\n",
    "4. **Azure AI Foundry** provides rich visualizations and historical tracking\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Iterate on agent prompts** based on evaluation results\n",
    "- **Add more test queries** to cover edge cases\n",
    "- **Compare evaluations** across different versions\n",
    "- **Set up automated evaluation** in your CI/CD pipeline\n",
    "\n",
    "Happy evaluating! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5d1b78",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî¨ Optional: Explore the Data\n",
    "\n",
    "Want to understand what data looks like at each step? Run the cells below to inspect the evaluation pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a937132",
   "metadata": {},
   "source": [
    "### View Test Queries\n",
    "\n",
    "These are the queries used to test the cooking agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce2aa881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Test Queries (10 total):\n",
      "\n",
      "1. Find me some pasta recipes\n",
      "2. What ingredients do I need for carbonara?\n",
      "3. I want to make something with chicken\n",
      "4. Suggest some quick recipes for dinner\n",
      "5. Do you have any soup recipes?\n",
      "6. What can I make for dessert?\n",
      "7. Show me vegetarian options\n",
      "8. I need ingredients for chicken stir fry\n",
      "9. What recipes use tomatoes?\n",
      "10. Give me some meat-based recipes\n"
     ]
    }
   ],
   "source": [
    "# Load and display test queries\n",
    "with open('test_queries.json', 'r', encoding='utf-8') as f:\n",
    "    queries_data = json.load(f)\n",
    "    queries = queries_data.get(\"queries\", [])\n",
    "\n",
    "print(f\"üìù Test Queries ({len(queries)} total):\\n\")\n",
    "for i, q in enumerate(queries, 1):\n",
    "    print(f\"{i}. {q['query']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b4f9b1",
   "metadata": {},
   "source": [
    "### View Agent Responses\n",
    "\n",
    "Let's look at a sample response from the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b322636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Sample Response:\n",
      "\n",
      "Query: Find me some pasta recipes\n",
      "\n",
      "Response: I found a delicious pasta recipe for you: Pasta Carbonara! It takes just 10 minutes to prep and 15 minutes to cook, making it a quick and tasty option. Would you like the full ingredient list or step-by-step cooking instructions for Pasta Carbonara? If you‚Äôre interested in more pasta recipes or have specific preferences (like vegetarian or gluten-free), let me know!\n",
      "\n",
      "Tool Calls: 1 calls\n",
      "\n",
      "Tool Call Details:\n",
      "  1. search_recipes({'query': 'pasta'})\n"
     ]
    }
   ],
   "source": [
    "# Load and display a sample response\n",
    "with open('test_responses.json', 'r', encoding='utf-8') as f:\n",
    "    responses_data = json.load(f)\n",
    "    responses = responses_data.get(\"responses\", [])\n",
    "\n",
    "# Show first response\n",
    "if responses:\n",
    "    sample = responses[0]\n",
    "    print(f\"üì§ Sample Response:\\n\")\n",
    "    print(f\"Query: {sample['query']}\")\n",
    "    print(f\"\\nResponse: {sample['response']}\")\n",
    "    print(f\"\\nTool Calls: {len(sample.get('tool_calls', []))} calls\")\n",
    "    \n",
    "    if sample.get('tool_calls'):\n",
    "        print(\"\\nTool Call Details:\")\n",
    "        for i, tc in enumerate(sample['tool_calls'], 1):\n",
    "            print(f\"  {i}. {tc.get('name', 'unknown')}({tc.get('arguments', {})})\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No responses found. Run 'python run_agent.py' first to collect responses.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5816ae57",
   "metadata": {},
   "source": [
    "### View Evaluation Data Format\n",
    "\n",
    "Here's what the formatted evaluation data looks like (JSONL format):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e3cf174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Evaluation Data Format (JSONL):\n",
      "\n",
      "{\n",
      "  \"query\": \"Find me some pasta recipes\",\n",
      "  \"response\": \"I found a delicious pasta recipe for you: Pasta Carbonara! It takes just 10 minutes to prep and 15 m...\",\n",
      "  \"tool_calls\": [\n",
      "    {\n",
      "      \"type\": \"tool_call\",\n",
      "      \"tool_call_id\": \"call_uy2o6m70jy00xddGS0wkA7HG\",\n",
      "      \"name\": \"search_recipes\",\n",
      "      \"arguments\": {\n",
      "        \"query\": \"pasta\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"tool_definitions_count\": 3\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Read and display first evaluation record\n",
    "import json\n",
    "\n",
    "print(\"üìÑ Evaluation Data Format (JSONL):\\n\")\n",
    "with open('evaluation_data.jsonl', 'r', encoding='utf-8') as f:\n",
    "    first_line = f.readline()\n",
    "    eval_record = json.loads(first_line)\n",
    "    \n",
    "    # Pretty print the structure\n",
    "    print(json.dumps({\n",
    "        \"query\": eval_record[\"query\"],\n",
    "        \"response\": eval_record[\"response\"][:100] + \"...\" if len(eval_record[\"response\"]) > 100 else eval_record[\"response\"],\n",
    "        \"tool_calls\": eval_record[\"tool_calls\"],\n",
    "        \"tool_definitions_count\": len(eval_record[\"tool_definitions\"])\n",
    "    }, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
